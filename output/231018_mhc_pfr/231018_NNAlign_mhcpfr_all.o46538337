INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld
+LD_GOLD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CPP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cpp
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC_NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
INFO: activate-gfortran_linux-64.sh made the following environmental changes:
+DEBUG_FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+DEBUG_FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+F77=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+F95=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-f95
+FC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+GFORTRAN=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-g++
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/pyscripts
"Starting PyScript"
Running iteration 1
torch.Size([106949, 37, 20])
PFR for this dataset completed
torch.Size([27332, 37, 20])
PFR for this dataset completed
torch.Size([17, 37, 20])
PFR for this dataset completed
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0458,	0.863
Epoch 50: valid loss, AUC:	0.0496,	0.864

Epoch 100: train loss, AUC:	0.0455,	0.864
Epoch 100: valid loss, AUC:	0.0500,	0.866

Epoch 150: train loss, AUC:	0.0454,	0.868
Epoch 150: valid loss, AUC:	0.0498,	0.864

Epoch 200: train loss, AUC:	0.0455,	0.867
Epoch 200: valid loss, AUC:	0.0497,	0.867

Epoch 250: train loss, AUC:	0.0458,	0.864
Epoch 250: valid loss, AUC:	0.0491,	0.858

Epoch 300: train loss, AUC:	0.0459,	0.862
Epoch 300: valid loss, AUC:	0.0503,	0.867

Epoch 350: train loss, AUC:	0.0458,	0.863
Epoch 350: valid loss, AUC:	0.0497,	0.861

Epoch 400: train loss, AUC:	0.0458,	0.865
Epoch 400: valid loss, AUC:	0.0493,	0.861

Epoch 450: train loss, AUC:	0.0458,	0.866
Epoch 450: valid loss, AUC:	0.0488,	0.866

Epoch 500: train loss, AUC:	0.0458,	0.865
Epoch 500: valid loss, AUC:	0.0505,	0.864
Shape of trainset: (tensor([[ -2.0000,  -2.0000,   2.0000,  ...,  -2.6667,  -0.3333,  -2.0000],
        [ -2.0000,  -3.0000,  -4.0000,  ...,  -2.6667,  -1.6667,  -1.6667],
        [ -2.0000,  -2.0000,   2.0000,  ...,  -2.0000,  -1.3333,  -2.0000],
        ...,
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	4.357e-02, best train AUC:	0.875
Best valid epoch: 118
Best valid loss :	5.022e-02, best valid AUC:	0.8748
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None/checkpoint_best_kcv_train_all5_correct_f01_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None/train_losses_kcv_train_all5_correct_f01_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None/valid_losses_kcv_train_all5_correct_f01_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None/train_metrics_kcv_train_all5_correct_f01_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None/valid_metrics_kcv_train_all5_correct_f01_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_1_231018_1131_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 303 minutes, 30 seconds.
Iteration 1 completed
Running iteration 2
torch.Size([104523, 37, 20])
PFR for this dataset completed
torch.Size([29758, 37, 20])
PFR for this dataset completed
torch.Size([17, 37, 20])
PFR for this dataset completed
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0455,	0.869
Epoch 50: valid loss, AUC:	0.0486,	0.823

Epoch 100: train loss, AUC:	0.0454,	0.866
Epoch 100: valid loss, AUC:	0.0477,	0.826

Epoch 150: train loss, AUC:	0.0452,	0.867
Epoch 150: valid loss, AUC:	0.0481,	0.833

Epoch 200: train loss, AUC:	0.0453,	0.867
Epoch 200: valid loss, AUC:	0.0485,	0.829

Epoch 250: train loss, AUC:	0.0454,	0.866
Epoch 250: valid loss, AUC:	0.0484,	0.825

Epoch 300: train loss, AUC:	0.0454,	0.866
Epoch 300: valid loss, AUC:	0.0480,	0.825

Epoch 350: train loss, AUC:	0.0454,	0.866
Epoch 350: valid loss, AUC:	0.0492,	0.827

Epoch 400: train loss, AUC:	0.0455,	0.866
Epoch 400: valid loss, AUC:	0.0482,	0.827

Epoch 450: train loss, AUC:	0.0455,	0.867
Epoch 450: valid loss, AUC:	0.0487,	0.832

Epoch 500: train loss, AUC:	0.0454,	0.870
Epoch 500: valid loss, AUC:	0.0491,	0.823
Shape of trainset: (tensor([[ -1.0000,  -3.0000,  -2.0000,  ...,  -2.6667,  -1.6667,  -0.6667],
        [ -1.0000,   3.0000,   0.0000,  ...,  -2.6667,  -1.6667,   0.3333],
        [ -2.0000,  -1.0000,  -2.0000,  ...,  -2.0000,  -1.3333,   0.0000],
        ...,
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	4.326e-02, best train AUC:	0.876
Best valid epoch: 2
Best valid loss :	4.703e-02, best valid AUC:	0.8514
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None/checkpoint_best_kcv_train_all5_correct_f02_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None/train_losses_kcv_train_all5_correct_f02_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None/valid_losses_kcv_train_all5_correct_f02_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None/train_metrics_kcv_train_all5_correct_f02_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None/valid_metrics_kcv_train_all5_correct_f02_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_2_231018_1635_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 303 minutes, 31 seconds.
Iteration 2 completed
Running iteration 3
torch.Size([105047, 37, 20])
PFR for this dataset completed
torch.Size([29234, 37, 20])
PFR for this dataset completed
torch.Size([17, 37, 20])
PFR for this dataset completed
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0461,	0.842
Epoch 50: valid loss, AUC:	0.0474,	0.843

Epoch 100: train loss, AUC:	0.0458,	0.846
Epoch 100: valid loss, AUC:	0.0473,	0.860

Epoch 150: train loss, AUC:	0.0457,	0.845
Epoch 150: valid loss, AUC:	0.0477,	0.838

Epoch 200: train loss, AUC:	0.0458,	0.844
Epoch 200: valid loss, AUC:	0.0481,	0.841

Epoch 250: train loss, AUC:	0.0458,	0.845
Epoch 250: valid loss, AUC:	0.0478,	0.844

Epoch 300: train loss, AUC:	0.0460,	0.844
Epoch 300: valid loss, AUC:	0.0474,	0.856

Epoch 350: train loss, AUC:	0.0461,	0.843
Epoch 350: valid loss, AUC:	0.0485,	0.836

Epoch 400: train loss, AUC:	0.0462,	0.844
Epoch 400: valid loss, AUC:	0.0484,	0.816

Epoch 450: train loss, AUC:	0.0461,	0.845
Epoch 450: valid loss, AUC:	0.0481,	0.845

Epoch 500: train loss, AUC:	0.0461,	0.844
Epoch 500: valid loss, AUC:	0.0478,	0.845
Shape of trainset: (tensor([[ -1.0000,  -3.0000,  -2.0000,  ...,  -2.6667,  -1.6667,  -0.6667],
        [ -1.0000,   3.0000,   0.0000,  ...,  -2.6667,  -1.6667,   0.3333],
        [ -2.0000,  -1.0000,  -2.0000,  ...,  -2.0000,  -1.3333,   0.0000],
        ...,
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	4.392e-02, best train AUC:	0.8554
Best valid epoch: 262
Best valid loss :	4.665e-02, best valid AUC:	0.8741
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None/checkpoint_best_kcv_train_all5_correct_f03_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None/train_losses_kcv_train_all5_correct_f03_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None/valid_losses_kcv_train_all5_correct_f03_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None/train_metrics_kcv_train_all5_correct_f03_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None/valid_metrics_kcv_train_all5_correct_f03_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_3_231018_2138_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 305 minutes, 32 seconds.
Iteration 3 completed
Running iteration 4
torch.Size([110618, 37, 20])
PFR for this dataset completed
torch.Size([23663, 37, 20])
PFR for this dataset completed
torch.Size([17, 37, 20])
PFR for this dataset completed
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0457,	0.856
Epoch 50: valid loss, AUC:	0.0473,	0.855

Epoch 100: train loss, AUC:	0.0454,	0.858
Epoch 100: valid loss, AUC:	0.0474,	0.847

Epoch 150: train loss, AUC:	0.0452,	0.858
Epoch 150: valid loss, AUC:	0.0474,	0.837

Epoch 200: train loss, AUC:	0.0452,	0.856
Epoch 200: valid loss, AUC:	0.0468,	0.843

Epoch 250: train loss, AUC:	0.0454,	0.855
Epoch 250: valid loss, AUC:	0.0479,	0.840

Epoch 300: train loss, AUC:	0.0455,	0.858
Epoch 300: valid loss, AUC:	0.0476,	0.854

Epoch 350: train loss, AUC:	0.0456,	0.856
Epoch 350: valid loss, AUC:	0.0479,	0.840

Epoch 400: train loss, AUC:	0.0455,	0.855
Epoch 400: valid loss, AUC:	0.0477,	0.836

Epoch 450: train loss, AUC:	0.0457,	0.853
Epoch 450: valid loss, AUC:	0.0472,	0.845

Epoch 500: train loss, AUC:	0.0456,	0.855
Epoch 500: valid loss, AUC:	0.0474,	0.837
Shape of trainset: (tensor([[ -1.0000,  -3.0000,  -2.0000,  ...,  -2.6667,  -1.6667,  -0.6667],
        [ -1.0000,   3.0000,   0.0000,  ...,  -2.6667,  -1.6667,   0.3333],
        [ -2.0000,  -1.0000,  -2.0000,  ...,  -2.0000,  -1.3333,   0.0000],
        ...,
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	4.361e-02, best train AUC:	0.8726
Best valid epoch: 5
Best valid loss :	4.879e-02, best valid AUC:	0.8672
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None/checkpoint_best_kcv_train_all5_correct_f04_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None/train_losses_kcv_train_all5_correct_f04_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None/valid_losses_kcv_train_all5_correct_f04_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None/train_metrics_kcv_train_all5_correct_f04_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None/valid_metrics_kcv_train_all5_correct_f04_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_4_231019_0244_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 302 minutes, 15 seconds.
Iteration 4 completed
Running iteration 5
torch.Size([109987, 37, 20])
PFR for this dataset completed
torch.Size([24294, 37, 20])
PFR for this dataset completed
torch.Size([17, 37, 20])
PFR for this dataset completed
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0457,	0.857
Epoch 50: valid loss, AUC:	0.0466,	0.875

Epoch 100: train loss, AUC:	0.0453,	0.856
Epoch 100: valid loss, AUC:	0.0466,	0.870

Epoch 150: train loss, AUC:	0.0450,	0.857
Epoch 150: valid loss, AUC:	0.0466,	0.880

Epoch 200: train loss, AUC:	0.0452,	0.855
Epoch 200: valid loss, AUC:	0.0464,	0.874

Epoch 250: train loss, AUC:	0.0451,	0.857
Epoch 250: valid loss, AUC:	0.0464,	0.879

Epoch 300: train loss, AUC:	0.0452,	0.856
Epoch 300: valid loss, AUC:	0.0465,	0.880

Epoch 350: train loss, AUC:	0.0453,	0.856
Epoch 350: valid loss, AUC:	0.0462,	0.883

Epoch 400: train loss, AUC:	0.0453,	0.858
Epoch 400: valid loss, AUC:	0.0462,	0.878

Epoch 450: train loss, AUC:	0.0452,	0.856
Epoch 450: valid loss, AUC:	0.0466,	0.873

Epoch 500: train loss, AUC:	0.0453,	0.857
Epoch 500: valid loss, AUC:	0.0463,	0.881
Shape of trainset: (tensor([[ -1.0000,  -3.0000,  -2.0000,  ...,  -2.6667,  -1.6667,  -0.6667],
        [ -1.0000,   3.0000,   0.0000,  ...,  -2.6667,  -1.6667,   0.3333],
        [ -2.0000,  -1.0000,  -2.0000,  ...,  -2.0000,  -1.3333,   0.0000],
        ...,
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],
        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	4.337e-02, best train AUC:	0.8662
Best valid epoch: 213
Best valid loss :	4.645e-02, best valid AUC:	0.8851
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None/checkpoint_best_kcv_train_all5_correct_f05_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None/train_losses_kcv_train_all5_correct_f05_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None/valid_losses_kcv_train_all5_correct_f05_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None/train_metrics_kcv_train_all5_correct_f05_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None/valid_metrics_kcv_train_all5_correct_f05_CSL_mhc_pfr_nostd_burn_60nh_all_KFold_5_231019_0746_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 302 minutes, 19 seconds.
Iteration 5 completed
Script finished
