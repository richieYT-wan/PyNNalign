/Users/riwa/Documents/code/PyNNalign/pyscripts
../output/PSEUDOSEQ_OLD_KFold_XX_240305_1749_None/
Filename: /Users/riwa/Documents/code/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   132    392.0 MiB    392.0 MiB           1       @profile
   133                                             def __init__(self, df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   134                                                          seq_col: str = 'sequence', target_col: str = 'target', pad_scale: float = None, indel: bool = False,
   135                                                          burnin_alphabet: str = 'ILVMFYW', feature_cols: list = ['placeholder'], add_pseudo_sequence=False,
   136                                                          pseudo_seq_col: str = 'pseudoseq', add_pfr=False, add_fr_len=False, add_pep_len=False, add_z=True,
   137                                                          burn_in=None):
   138                                                 # start = dt.now()
   139    392.0 MiB      0.0 MiB           1           super(NNAlignDatasetEFSinglePass, self).__init__()
   140                                                 # Encoding stuff
   141    392.0 MiB      0.0 MiB           1           if feature_cols is None:
   142    392.0 MiB      0.0 MiB           1               feature_cols = []
   143                                                 # Filter out sequences longer than max_len
   144    393.3 MiB      1.3 MiB           1           df['len'] = df[seq_col].apply(len)
   145    399.5 MiB      6.2 MiB           1           df = df.query('len<=@max_len')
   146                                                 # Then, if indel is False, filter out sequences shorter than windowsize (ex: 8mers for WS=9)
   147    399.5 MiB      0.0 MiB           1           if not indel:
   148                                                     df = df.query('len>=@window_size')
   149    399.5 MiB      0.0 MiB           1           self.burn_in_flag = False
   150    399.5 MiB      0.0 MiB           1           matrix_dim = 20
   151                                                 # query_time = dt.now()
   152    686.2 MiB    286.7 MiB           1           x = encode_batch(df[seq_col], max_len, encoding, pad_scale)
   153    686.5 MiB      0.3 MiB           1           y = torch.from_numpy(df[target_col].values).float().view(-1, 1)
   154                                                 # encode_time = dt.now()
   155                                                 # Creating the mask to allow selection of kmers without padding
   156    686.5 MiB      0.0 MiB           1           len_mask = torch.from_numpy(df['len'].values)
   157    687.9 MiB      1.4 MiB           1           x_mask = len_mask - window_size
   158    692.1 MiB      4.2 MiB           1           range_tensor = torch.arange(max_len - window_size + 1).unsqueeze(0).repeat(len(x), 1)
   159                                                 # Mask for Kmers + padding
   160    695.3 MiB      3.2 MiB           1           x_mask = (range_tensor <= x_mask.unsqueeze(1)).float().unsqueeze(-1)
   161                                                 # Expand the kmers windows for base sequence without indels
   162    695.6 MiB      0.1 MiB           2           x = x.unfold(1, window_size, 1).transpose(2, 3) \
   163    695.6 MiB      0.1 MiB           1               .reshape(len(x), max_len - window_size + 1, window_size, matrix_dim)
   164                                                 # Creating indels window and mask 
   165    695.6 MiB      0.0 MiB           1           if indel:
   166    852.1 MiB    156.5 MiB           1               x_indel = batch_insertion_deletion(df[seq_col], max_len, encoding, pad_scale, window_size)
   167                                                     # remove padding from indel windows
   168    852.8 MiB      0.7 MiB           1               x_indel = x_indel[:, :, :window_size, :]
   169   1083.6 MiB    230.8 MiB           1               indel_mask = batch_indel_mask(len_mask, window_size)
   170   3078.3 MiB   1994.7 MiB           1               x = torch.cat([x, x_indel], dim=1)
   171   3097.7 MiB     19.4 MiB           1               x_mask = torch.cat([x_mask, indel_mask], dim=1)
   172                                         
   173   3097.7 MiB      0.0 MiB           1           if burn_in is not None:
   174                                                     # Creating another mask for the burn-in period+bool flag switch
   175   3100.8 MiB  -3272.7 MiB           3               self.burn_in_mask = _get_burnin_mask_batch(df[seq_col].values, max_len, window_size,
   176   3100.8 MiB  -1637.8 MiB           2                                                          burnin_alphabet).unsqueeze(-1)
   177   1462.9 MiB  -1637.9 MiB           1               if indel:
   178   1462.9 MiB  -2173.6 MiB           3                   indel_burn_in_mask = _get_indel_burnin_mask_batch(df[seq_col].values, window_size,
   179   1462.9 MiB  -1086.8 MiB           2                                                                     burnin_alphabet).unsqueeze(-1)
   180    388.7 MiB  -1074.2 MiB           1                   self.burn_in_mask = torch.cat([self.burn_in_mask, indel_burn_in_mask], dim=1)
   181                                         
   182                                                 # Expand and unfold the sub kmers and the target to match the shape ; contiguous to allow for view operations
   183    389.1 MiB      0.4 MiB           1           self.x_tensor = x.flatten(2, 3).contiguous()
   184    389.1 MiB      0.0 MiB           1           self.x_mask = x_mask
   185                                         
   186                                                 # kmer_time = dt.now()
   187    389.1 MiB      0.0 MiB           1           self.y = y.contiguous()
   188    389.2 MiB      0.1 MiB           1           self.x_features = torch.empty((len(x),))
   189                                                 # Add extra features
   190    389.2 MiB      0.0 MiB           1           if len(feature_cols) > 0:
   191                                                     # TODO: When you add more features you need to concatenate to x_pseudosequence and save it to self.x_features
   192                                                     # these are NUMERICAL FEATURES like %Rank, expression, etc. of shape (N, len(feature_cols))
   193                                                     # x_features = torch.from_numpy(df[feature_cols].values).float()
   194                                         
   195                                                     self.extra_features_flag = True
   196                                                 else:
   197    389.2 MiB      0.0 MiB           1               self.extra_features_flag = False
   198                                         
   199                                                 #  TODO dictmap for 9mer look-up and see if how many duplicated and can we save memory
   200                                                 #
   201    389.2 MiB      0.0 MiB           1           if add_pseudo_sequence:
   202    883.5 MiB    494.3 MiB           1               x_pseudoseq = encode_batch(df[pseudo_seq_col], 34, encoding, pad_scale)
   203    883.5 MiB      0.0 MiB           1               x_pseudoseq = x_pseudoseq.flatten(start_dim=1)
   204    883.5 MiB      0.0 MiB           1               self.x_features = x_pseudoseq
   205    883.5 MiB      0.0 MiB           1               self.extra_features_flag = True
   206                                                     # ps_time = dt.now()
   207    883.5 MiB      0.0 MiB           1           if add_pfr:
   208                                                     x_pfr = PFR_calculation(df[seq_col], self.x_mask, max_len, window_size)
   209                                                     self.x_tensor = torch.cat([self.x_tensor, x_pfr], dim=2)
