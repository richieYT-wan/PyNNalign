INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld
+LD_GOLD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CPP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cpp
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC_NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
INFO: activate-gfortran_linux-64.sh made the following environmental changes:
+DEBUG_FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+DEBUG_FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+F77=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+F95=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-f95
+FC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+GFORTRAN=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-g++
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/pyscripts
"Starting PyScript"
Running iteration 1
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    252.7 MiB    252.7 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    252.7 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    252.7 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3724.4 MiB   3471.7 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3724.4 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3724.4 MiB      0.0 MiB           1       if return_dataset:
   224   3724.4 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3724.4 MiB   3724.4 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3724.4 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3724.4 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4438.5 MiB    714.1 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4438.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4438.5 MiB      0.0 MiB           1       if return_dataset:
   224   4438.5 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4438.5 MiB   4438.5 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4438.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4438.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4438.5 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4438.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4438.5 MiB      0.0 MiB           1       if return_dataset:
   224   4438.5 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 1500 training cycles

Epoch 150: train loss, AUC:	0.0248,	0.944
Epoch 150: valid loss, AUC:	0.0362,	0.882

Epoch 300: train loss, AUC:	0.0232,	0.950
Epoch 300: valid loss, AUC:	0.0365,	0.876

Epoch 450: train loss, AUC:	0.0223,	0.952
Epoch 450: valid loss, AUC:	0.0374,	0.878

Epoch 600: train loss, AUC:	0.0218,	0.953
Epoch 600: valid loss, AUC:	0.0368,	0.867

Epoch 750: train loss, AUC:	0.0216,	0.954
Epoch 750: valid loss, AUC:	0.0369,	0.878

Epoch 900: train loss, AUC:	0.0213,	0.954
Epoch 900: valid loss, AUC:	0.0378,	0.879

Epoch 1050: train loss, AUC:	0.0209,	0.955
Epoch 1050: valid loss, AUC:	0.0376,	0.874

Epoch 1200: train loss, AUC:	0.0209,	0.955
Epoch 1200: valid loss, AUC:	0.0379,	0.870

Epoch 1350: train loss, AUC:	0.0207,	0.955
Epoch 1350: valid loss, AUC:	0.0390,	0.855

Epoch 1500: train loss, AUC:	0.0207,	0.955
Epoch 1500: valid loss, AUC:	0.0385,	0.861
End of training cycles
Best train loss:	2.057e-02, best train AUC:	0.9573
Best valid epoch: 1500
Best valid loss :	3.855e-02, best valid AUC:	0.8608
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None/checkpoint_best_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4438.5 MiB   4438.5 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4438.5 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4438.5 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4438.5 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4438.5 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4438.5 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   4444.7 MiB     -0.9 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   4444.7 MiB      5.2 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   4444.7 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   4444.7 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   4444.8 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   4444.8 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   4444.8 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   4474.0 MiB  -4690.5 MiB        1501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   4474.0 MiB  -4634.6 MiB        1500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   4474.0 MiB  -4826.0 MiB        1500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   4474.0 MiB  -4716.2 MiB        1500           train_metrics.append(train_metric)
   226   4474.0 MiB  -4716.2 MiB        1500           valid_metrics.append(valid_metric)
   227   4474.0 MiB  -4716.2 MiB        1500           train_losses.append(train_loss)
   228   4474.0 MiB  -4716.2 MiB        1500           valid_losses.append(valid_loss)
   229   4474.0 MiB  -4716.2 MiB        1500           if e % (n_epochs // 10) == 0:
   230                                                     # tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231                                                     # tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   4474.0 MiB    -26.8 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   4474.0 MiB    -25.9 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   4474.0 MiB  -4715.3 MiB        1500           if e == n_epochs:
   240   4448.3 MiB    -25.8 MiB           1               best_epoch = e
   241   4448.3 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   4448.3 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   4448.3 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   4448.3 MiB    -25.8 MiB           1       print(f'End of training cycles')
   246   4448.3 MiB      0.0 MiB        1503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   4448.3 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   4448.3 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   4448.3 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   4448.3 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   4448.3 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None/train_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None/valid_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None/train_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None/valid_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_1_231130_1206_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 290 minutes, 27 seconds.
Iteration 1 completed
Running iteration 2
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    258.5 MiB    258.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    258.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    258.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   3652.3 MiB   3393.8 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   3652.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   3652.3 MiB      0.0 MiB           1       if return_dataset:
   244   3652.3 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   3652.3 MiB   3652.3 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   3652.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   3652.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4501.5 MiB    849.2 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4501.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4501.5 MiB      0.0 MiB           1       if return_dataset:
   244   4501.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   4501.5 MiB   4501.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   4501.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   4501.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4501.6 MiB      0.1 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4501.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4501.6 MiB      0.0 MiB           1       if return_dataset:
   244   4501.6 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 1500 training cycles

Epoch 150: train loss, AUC:	0.0240,	0.943
Epoch 150: valid loss, AUC:	0.0361,	0.900

Epoch 300: train loss, AUC:	0.0225,	0.950
Epoch 300: valid loss, AUC:	0.0383,	0.896

Epoch 450: train loss, AUC:	0.0217,	0.952
Epoch 450: valid loss, AUC:	0.0376,	0.891

Epoch 600: train loss, AUC:	0.0211,	0.955
Epoch 600: valid loss, AUC:	0.0383,	0.894

Epoch 750: train loss, AUC:	0.0206,	0.957
Epoch 750: valid loss, AUC:	0.0407,	0.886

Epoch 900: train loss, AUC:	0.0204,	0.958
Epoch 900: valid loss, AUC:	0.0393,	0.886

Epoch 1050: train loss, AUC:	0.0201,	0.958
Epoch 1050: valid loss, AUC:	0.0389,	0.886

Epoch 1200: train loss, AUC:	0.0200,	0.960
Epoch 1200: valid loss, AUC:	0.0392,	0.894

Epoch 1350: train loss, AUC:	0.0198,	0.959
Epoch 1350: valid loss, AUC:	0.0384,	0.896

Epoch 1500: train loss, AUC:	0.0198,	0.959
Epoch 1500: valid loss, AUC:	0.0395,	0.897
End of training cycles
Best train loss:	1.964e-02, best train AUC:	0.9609
Best valid epoch: 1500
Best valid loss :	3.948e-02, best valid AUC:	0.8974
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None/checkpoint_best_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4501.6 MiB   4501.6 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4501.6 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4501.6 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4501.6 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4501.6 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4501.6 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4514.4 MiB     -0.5 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4514.4 MiB     12.1 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4514.3 MiB     -0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4514.3 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4514.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4514.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4514.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   4560.6 MiB -33371.5 MiB        1501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   4561.3 MiB -32708.7 MiB        1500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   4560.6 MiB -34433.4 MiB        1500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   4560.6 MiB -33388.9 MiB        1500           train_metrics.append(train_metric)
   225   4560.6 MiB -33388.9 MiB        1500           valid_metrics.append(valid_metric)
   226   4560.6 MiB -33388.9 MiB        1500           train_losses.append(train_loss)
   227   4560.6 MiB -33388.9 MiB        1500           valid_losses.append(valid_loss)
   228                                         
   229   4560.6 MiB -33388.9 MiB        1500           if (n_epochs)>10:
   230   4560.6 MiB -33388.9 MiB        1500               if e % (n_epochs // 10) == 0:
   231   4559.0 MiB   -256.4 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232   4559.0 MiB   -243.8 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238   4560.6 MiB -33376.3 MiB        1500           if e == n_epochs:
   239   4543.2 MiB    -17.4 MiB           1               best_epoch = e
   240   4543.2 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241   4543.2 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242   4543.2 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244   4543.2 MiB    -17.4 MiB           1       print(f'End of training cycles')
   245   4543.2 MiB      0.0 MiB        1503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246   4543.2 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247   4543.2 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248   4543.2 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249   4543.3 MiB      0.1 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250   4543.3 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None/train_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None/valid_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None/train_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None/valid_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_2_231130_1656_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 259 minutes, 21 seconds.
Iteration 2 completed
Running iteration 3
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    257.1 MiB    257.1 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    257.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    257.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   3669.0 MiB   3412.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   3669.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   3669.0 MiB      0.0 MiB           1       if return_dataset:
   244   3669.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   3669.0 MiB   3669.0 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   3669.0 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   3669.0 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4502.5 MiB    833.5 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4502.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4502.5 MiB      0.0 MiB           1       if return_dataset:
   244   4502.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   4502.5 MiB   4502.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   4502.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   4502.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4503.0 MiB      0.5 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4503.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4503.0 MiB      0.0 MiB           1       if return_dataset:
   244   4503.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 1500 training cycles

Epoch 150: train loss, AUC:	0.0244,	0.935
Epoch 150: valid loss, AUC:	0.0334,	0.934

Epoch 300: train loss, AUC:	0.0230,	0.940
Epoch 300: valid loss, AUC:	0.0355,	0.930

Epoch 450: train loss, AUC:	0.0221,	0.944
Epoch 450: valid loss, AUC:	0.0347,	0.932

Epoch 600: train loss, AUC:	0.0216,	0.944
Epoch 600: valid loss, AUC:	0.0350,	0.934

Epoch 750: train loss, AUC:	0.0212,	0.944
Epoch 750: valid loss, AUC:	0.0355,	0.926

Epoch 900: train loss, AUC:	0.0212,	0.947
Epoch 900: valid loss, AUC:	0.0360,	0.929

Epoch 1050: train loss, AUC:	0.0208,	0.947
Epoch 1050: valid loss, AUC:	0.0353,	0.927

Epoch 1200: train loss, AUC:	0.0206,	0.949
Epoch 1200: valid loss, AUC:	0.0357,	0.926

Epoch 1350: train loss, AUC:	0.0205,	0.948
Epoch 1350: valid loss, AUC:	0.0361,	0.928

Epoch 1500: train loss, AUC:	0.0203,	0.949
Epoch 1500: valid loss, AUC:	0.0364,	0.926
End of training cycles
Best train loss:	2.027e-02, best train AUC:	0.9508
Best valid epoch: 1500
Best valid loss :	3.638e-02, best valid AUC:	0.9258
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None/checkpoint_best_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4503.0 MiB   4503.0 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4503.0 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4503.0 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4503.0 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4503.0 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4503.0 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4511.3 MiB     -2.2 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4511.3 MiB      6.0 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4511.3 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4511.3 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4511.4 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4511.4 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4511.4 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   4513.9 MiB   -291.2 MiB        1501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   4516.6 MiB    836.5 MiB        1500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   4513.9 MiB  -2224.4 MiB        1500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   4513.9 MiB   -291.3 MiB        1500           train_metrics.append(train_metric)
   225   4513.9 MiB   -291.3 MiB        1500           valid_metrics.append(valid_metric)
   226   4513.9 MiB   -291.3 MiB        1500           train_losses.append(train_loss)
   227   4513.9 MiB   -291.3 MiB        1500           valid_losses.append(valid_loss)
   228                                         
   229   4513.9 MiB   -291.3 MiB        1500           if (n_epochs)>10:
   230   4513.9 MiB   -291.3 MiB        1500               if e % (n_epochs // 10) == 0:
   231   4513.9 MiB     -1.6 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232   4513.9 MiB     -0.2 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238   4513.9 MiB   -289.9 MiB        1500           if e == n_epochs:
   239   4513.9 MiB     -0.0 MiB           1               best_epoch = e
   240   4513.9 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241   4513.9 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242   4513.9 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244   4513.9 MiB     -0.0 MiB           1       print(f'End of training cycles')
   245   4513.9 MiB      0.0 MiB        1503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246   4513.9 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247   4513.9 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248   4513.9 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249   4514.1 MiB      0.2 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250   4514.1 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None/train_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None/valid_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None/train_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None/valid_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_3_231130_2116_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 282 minutes, 15 seconds.
Iteration 3 completed
Running iteration 4
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    257.9 MiB    257.9 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    257.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    257.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   3850.3 MiB   3592.4 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   3850.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   3850.3 MiB      0.0 MiB           1       if return_dataset:
   244   3850.3 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   3850.3 MiB   3850.3 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   3850.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   3850.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4465.5 MiB    615.1 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4465.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4465.5 MiB      0.0 MiB           1       if return_dataset:
   244   4465.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   4465.5 MiB   4465.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   4465.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   4465.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4465.5 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4465.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4465.5 MiB      0.0 MiB           1       if return_dataset:
   244   4465.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 1500 training cycles

Epoch 150: train loss, AUC:	0.0233,	0.943
Epoch 150: valid loss, AUC:	0.0384,	0.887

Epoch 300: train loss, AUC:	0.0218,	0.949
Epoch 300: valid loss, AUC:	0.0393,	0.883

Epoch 450: train loss, AUC:	0.0211,	0.951
Epoch 450: valid loss, AUC:	0.0407,	0.900

Epoch 600: train loss, AUC:	0.0206,	0.952
Epoch 600: valid loss, AUC:	0.0403,	0.890

Epoch 750: train loss, AUC:	0.0202,	0.954
Epoch 750: valid loss, AUC:	0.0409,	0.894

Epoch 900: train loss, AUC:	0.0201,	0.953
Epoch 900: valid loss, AUC:	0.0407,	0.889

Epoch 1050: train loss, AUC:	0.0199,	0.953
Epoch 1050: valid loss, AUC:	0.0411,	0.895

Epoch 1200: train loss, AUC:	0.0197,	0.954
Epoch 1200: valid loss, AUC:	0.0412,	0.886

Epoch 1350: train loss, AUC:	0.0195,	0.955
Epoch 1350: valid loss, AUC:	0.0410,	0.894

Epoch 1500: train loss, AUC:	0.0195,	0.955
Epoch 1500: valid loss, AUC:	0.0446,	0.890
End of training cycles
Best train loss:	1.932e-02, best train AUC:	0.9569
Best valid epoch: 1500
Best valid loss :	4.457e-02, best valid AUC:	0.8897
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None/checkpoint_best_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4465.5 MiB   4465.5 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4465.5 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4465.5 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4465.5 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4465.5 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4465.5 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   6172.8 MiB     -0.2 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   6172.8 MiB   1707.0 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   6172.8 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   6172.8 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   6172.9 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   6172.9 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   6172.9 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   8510.0 MiB -1452150.7 MiB        1501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   8509.9 MiB -1450431.5 MiB        1500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   8510.0 MiB -1477081.3 MiB        1500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   8510.0 MiB -1454528.8 MiB        1500           train_metrics.append(train_metric)
   225   8510.0 MiB -1454528.8 MiB        1500           valid_metrics.append(valid_metric)
   226   8510.0 MiB -1454528.8 MiB        1500           train_losses.append(train_loss)
   227   8510.0 MiB -1454528.8 MiB        1500           valid_losses.append(valid_loss)
   228                                         
   229   8510.0 MiB -1454528.8 MiB        1500           if (n_epochs)>10:
   230   8510.0 MiB -1454528.8 MiB        1500               if e % (n_epochs // 10) == 0:
   231   6132.0 MiB -10355.4 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232   6132.0 MiB     -0.7 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238   8510.0 MiB -1444174.1 MiB        1500           if e == n_epochs:
   239   6132.0 MiB  -2378.0 MiB           1               best_epoch = e
   240   6132.0 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241   6132.0 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242   6132.0 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244   6132.0 MiB  -2378.0 MiB           1       print(f'End of training cycles')
   245   6132.0 MiB      0.0 MiB        1503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246   6132.0 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247   6132.0 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248   6132.0 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249   6132.0 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250   6132.0 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None/train_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None/valid_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None/train_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None/valid_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_4_231201_0158_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 217 minutes, 31 seconds.
Iteration 4 completed
Running iteration 5
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    255.2 MiB    255.2 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    255.2 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    255.2 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   3829.8 MiB   3574.6 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   3829.8 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   3829.8 MiB      0.0 MiB           1       if return_dataset:
   244   3829.8 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   3829.8 MiB   3829.8 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   3829.8 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   3829.8 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4463.0 MiB    633.2 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4463.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4463.0 MiB      0.0 MiB           1       if return_dataset:
   244   4463.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   4463.0 MiB   4463.0 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   4463.0 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   4463.0 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   4463.0 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   4463.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   4463.0 MiB      0.0 MiB           1       if return_dataset:
   244   4463.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 1500 training cycles

Epoch 150: train loss, AUC:	0.0240,	0.940
Epoch 150: valid loss, AUC:	0.0361,	0.915

Epoch 300: train loss, AUC:	0.0226,	0.943
Epoch 300: valid loss, AUC:	0.0372,	0.914

Epoch 450: train loss, AUC:	0.0216,	0.946
Epoch 450: valid loss, AUC:	0.0370,	0.910

Epoch 600: train loss, AUC:	0.0212,	0.950
Epoch 600: valid loss, AUC:	0.0384,	0.910

Epoch 750: train loss, AUC:	0.0209,	0.949
Epoch 750: valid loss, AUC:	0.0379,	0.907

Epoch 900: train loss, AUC:	0.0207,	0.949
Epoch 900: valid loss, AUC:	0.0406,	0.907

Epoch 1050: train loss, AUC:	0.0204,	0.951
Epoch 1050: valid loss, AUC:	0.0382,	0.907

Epoch 1200: train loss, AUC:	0.0204,	0.952
Epoch 1200: valid loss, AUC:	0.0391,	0.907

Epoch 1350: train loss, AUC:	0.0201,	0.952
Epoch 1350: valid loss, AUC:	0.0384,	0.905

Epoch 1500: train loss, AUC:	0.0200,	0.952
Epoch 1500: valid loss, AUC:	0.0386,	0.901
End of training cycles
Best train loss:	1.992e-02, best train AUC:	0.9538
Best valid epoch: 1500
Best valid loss :	3.856e-02, best valid AUC:	0.9007
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None/checkpoint_best_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4463.0 MiB   4463.0 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4463.0 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4463.0 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4463.0 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4463.0 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4463.0 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4878.0 MiB     -4.8 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4878.0 MiB    410.2 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4878.0 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4878.0 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4878.1 MiB      0.0 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4878.1 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4878.1 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   5324.3 MiB -609647.4 MiB        1501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   5533.9 MiB -607149.1 MiB        1500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   5324.3 MiB -884654.3 MiB        1500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   5324.3 MiB -610083.5 MiB        1500           train_metrics.append(train_metric)
   225   5324.3 MiB -610083.5 MiB        1500           valid_metrics.append(valid_metric)
   226   5324.3 MiB -610083.5 MiB        1500           train_losses.append(train_loss)
   227   5324.3 MiB -610083.5 MiB        1500           valid_losses.append(valid_loss)
   228                                         
   229   5324.3 MiB -610083.5 MiB        1500           if (n_epochs)>10:
   230   5324.3 MiB -610083.5 MiB        1500               if e % (n_epochs // 10) == 0:
   231   4888.3 MiB  -4147.3 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232   4888.3 MiB   -100.9 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238   5324.3 MiB -606037.1 MiB        1500           if e == n_epochs:
   239   4888.3 MiB   -436.0 MiB           1               best_epoch = e
   240   4888.3 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241   4888.3 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242   4888.3 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244   4888.3 MiB   -436.0 MiB           1       print(f'End of training cycles')
   245   4888.3 MiB      0.0 MiB        1503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246   4888.3 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247   4888.3 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248   4888.3 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249   4888.3 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250   4888.3 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None/train_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None/valid_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None/train_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None/valid_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_ne1500_last_KFold_5_231201_0536_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 273 minutes, 52 seconds.
Iteration 5 completed
Script finished
