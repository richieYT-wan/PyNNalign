INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld
+LD_GOLD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CPP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cpp
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC_NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
INFO: activate-gfortran_linux-64.sh made the following environmental changes:
+DEBUG_FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+DEBUG_FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+F77=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+F95=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-f95
+FC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+GFORTRAN=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-g++
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/pyscripts
"Starting PyScript"
Running iteration 1
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    253.6 MiB    253.6 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    253.6 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    253.6 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3727.1 MiB   3473.5 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3727.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3727.1 MiB      0.0 MiB           1       if return_dataset:
   224   3727.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3727.1 MiB   3727.1 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3727.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3727.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4439.4 MiB    712.3 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4439.4 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4439.4 MiB      0.0 MiB           1       if return_dataset:
   224   4439.4 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4439.4 MiB   4439.4 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4439.4 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4439.4 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4439.4 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4439.4 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4439.4 MiB      0.0 MiB           1       if return_dataset:
   224   4439.4 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0279,	0.934
Epoch 50: valid loss, AUC:	0.0349,	0.873

Epoch 100: train loss, AUC:	0.0260,	0.941
Epoch 100: valid loss, AUC:	0.0353,	0.881

Epoch 150: train loss, AUC:	0.0248,	0.944
Epoch 150: valid loss, AUC:	0.0362,	0.882

Epoch 200: train loss, AUC:	0.0241,	0.947
Epoch 200: valid loss, AUC:	0.0358,	0.867

Epoch 250: train loss, AUC:	0.0237,	0.948
Epoch 250: valid loss, AUC:	0.0361,	0.865

Epoch 300: train loss, AUC:	0.0232,	0.950
Epoch 300: valid loss, AUC:	0.0365,	0.876

Epoch 350: train loss, AUC:	0.0227,	0.950
Epoch 350: valid loss, AUC:	0.0365,	0.870

Epoch 400: train loss, AUC:	0.0225,	0.951
Epoch 400: valid loss, AUC:	0.0371,	0.870

Epoch 450: train loss, AUC:	0.0223,	0.952
Epoch 450: valid loss, AUC:	0.0374,	0.878

Epoch 500: train loss, AUC:	0.0220,	0.952
Epoch 500: valid loss, AUC:	0.0366,	0.866
End of training cycles
Best train loss:	2.198e-02, best train AUC:	0.9533
Best valid epoch: 66
Best valid loss :	3.607e-02, best valid AUC:	0.8922
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None/checkpoint_best_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4439.4 MiB   4439.4 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4439.4 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4439.4 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4439.4 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4439.4 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4439.4 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   6016.5 MiB     -3.2 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   6016.5 MiB   1573.9 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   6016.5 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   6016.5 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   6016.7 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   6016.7 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   6016.7 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   9191.5 MiB -2230733.1 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222  10188.0 MiB -2229836.7 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   9191.5 MiB -2798903.6 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   9191.5 MiB -2249683.6 MiB         500           train_metrics.append(train_metric)
   225   9191.5 MiB -2249683.6 MiB         500           valid_metrics.append(valid_metric)
   226   9191.5 MiB -2249683.6 MiB         500           train_losses.append(train_loss)
   227   9191.5 MiB -2249683.6 MiB         500           valid_losses.append(valid_loss)
   228   9191.5 MiB -2249683.6 MiB         500           if e % (n_epochs // 10) == 0:
   229   5105.7 MiB -46615.4 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   230   5105.7 MiB  -4477.6 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   231                                         
   232                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   233                                                 # performance for whatever reasons
   234   9191.5 MiB -2207545.8 MiB         500           if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc)\
   235   9191.5 MiB -2233627.1 MiB         491                         or valid_metric['auc'] > best_val_auc):
   236   5402.3 MiB -26245.3 MiB          11               best_epoch = e
   237   5402.3 MiB  -7299.4 MiB          11               best_val_loss = valid_loss
   238   5402.3 MiB  -7299.4 MiB          11               best_val_auc = valid_metric['auc']
   239   5402.3 MiB  -7299.4 MiB          11               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   240                                         
   241   4466.4 MiB  -4725.1 MiB           1       print(f'End of training cycles')
   242   4466.4 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   243   4466.4 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   244   4466.4 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   245   4466.4 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   246   4466.4 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   247   4466.4 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None/train_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None/valid_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None/train_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None/valid_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_all_KFold_1_231125_1045_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 105 minutes, 13 seconds.
Iteration 1 completed
Running iteration 2
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    253.8 MiB    253.8 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    253.8 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    253.8 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3650.9 MiB   3397.1 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3650.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3650.9 MiB      0.0 MiB           1       if return_dataset:
   224   3650.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3650.9 MiB   3650.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3650.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3650.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4496.2 MiB    845.3 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4496.2 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4496.2 MiB      0.0 MiB           1       if return_dataset:
   224   4496.2 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4496.2 MiB   4496.2 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4496.2 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4496.2 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4496.2 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4496.2 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4496.2 MiB      0.0 MiB           1       if return_dataset:
   224   4496.2 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0271,	0.932
Epoch 50: valid loss, AUC:	0.0366,	0.888

Epoch 100: train loss, AUC:	0.0253,	0.942
Epoch 100: valid loss, AUC:	0.0364,	0.896

Epoch 150: train loss, AUC:	0.0240,	0.943
Epoch 150: valid loss, AUC:	0.0361,	0.900

Epoch 200: train loss, AUC:	0.0233,	0.947
Epoch 200: valid loss, AUC:	0.0367,	0.893

Epoch 250: train loss, AUC:	0.0229,	0.948
Epoch 250: valid loss, AUC:	0.0369,	0.889

Epoch 300: train loss, AUC:	0.0225,	0.950
Epoch 300: valid loss, AUC:	0.0383,	0.896

Epoch 350: train loss, AUC:	0.0222,	0.951
Epoch 350: valid loss, AUC:	0.0379,	0.899

Epoch 400: train loss, AUC:	0.0219,	0.951
Epoch 400: valid loss, AUC:	0.0374,	0.894

Epoch 450: train loss, AUC:	0.0217,	0.952
Epoch 450: valid loss, AUC:	0.0376,	0.891

Epoch 500: train loss, AUC:	0.0214,	0.952
Epoch 500: valid loss, AUC:	0.0379,	0.890
End of training cycles
Best train loss:	2.135e-02, best train AUC:	0.9548
Best valid epoch: 292
Best valid loss :	3.686e-02, best valid AUC:	0.9029
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None/checkpoint_best_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4496.2 MiB   4496.2 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4496.2 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4496.2 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4496.2 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4496.2 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4496.2 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4531.7 MiB    -16.6 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4531.7 MiB     18.9 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4531.7 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4531.7 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4531.7 MiB      0.0 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4531.7 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4531.7 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   6756.4 MiB -561976.9 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   7018.7 MiB -529491.4 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   6756.4 MiB -1176680.0 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   6756.4 MiB -566416.5 MiB         500           train_metrics.append(train_metric)
   225   6756.4 MiB -566416.5 MiB         500           valid_metrics.append(valid_metric)
   226   6756.4 MiB -566416.5 MiB         500           train_losses.append(train_loss)
   227   6756.4 MiB -566416.5 MiB         500           valid_losses.append(valid_loss)
   228   6756.4 MiB -566416.5 MiB         500           if e % (n_epochs // 10) == 0:
   229   4836.8 MiB -12484.5 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   230   4836.8 MiB   -514.7 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   231                                         
   232                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   233                                                 # performance for whatever reasons
   234   6756.4 MiB -554446.7 MiB         500           if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc)\
   235   6756.4 MiB -564052.7 MiB         490                         or valid_metric['auc'] > best_val_auc):
   236   4783.5 MiB  -5384.4 MiB          14               best_epoch = e
   237   4783.5 MiB   -946.4 MiB          14               best_val_loss = valid_loss
   238   4783.5 MiB   -946.4 MiB          14               best_val_auc = valid_metric['auc']
   239   4783.5 MiB   -946.4 MiB          14               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   240                                         
   241   4507.2 MiB  -2249.2 MiB           1       print(f'End of training cycles')
   242   4507.2 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   243   4507.2 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   244   4507.2 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   245   4507.2 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   246   4507.4 MiB      0.2 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   247   4507.4 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None/train_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None/valid_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None/train_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None/valid_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_all_KFold_2_231125_1231_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 90 minutes, 48 seconds.
Iteration 2 completed
Running iteration 3
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    257.1 MiB    257.1 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    257.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    257.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3669.9 MiB   3412.8 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3669.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3669.9 MiB      0.0 MiB           1       if return_dataset:
   224   3669.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3669.9 MiB   3669.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3669.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3669.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4501.8 MiB    831.9 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4501.8 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4501.8 MiB      0.0 MiB           1       if return_dataset:
   224   4501.8 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4501.8 MiB   4501.8 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4501.8 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4501.8 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4501.8 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4501.8 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4501.8 MiB      0.0 MiB           1       if return_dataset:
   224   4501.8 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0277,	0.921
Epoch 50: valid loss, AUC:	0.0330,	0.934

Epoch 100: train loss, AUC:	0.0256,	0.930
Epoch 100: valid loss, AUC:	0.0328,	0.932

Epoch 150: train loss, AUC:	0.0244,	0.935
Epoch 150: valid loss, AUC:	0.0334,	0.934

Epoch 200: train loss, AUC:	0.0239,	0.934
Epoch 200: valid loss, AUC:	0.0343,	0.927

Epoch 250: train loss, AUC:	0.0233,	0.938
Epoch 250: valid loss, AUC:	0.0346,	0.934

Epoch 300: train loss, AUC:	0.0230,	0.940
Epoch 300: valid loss, AUC:	0.0355,	0.930

Epoch 350: train loss, AUC:	0.0227,	0.943
Epoch 350: valid loss, AUC:	0.0341,	0.930

Epoch 400: train loss, AUC:	0.0224,	0.942
Epoch 400: valid loss, AUC:	0.0348,	0.926

Epoch 450: train loss, AUC:	0.0221,	0.944
Epoch 450: valid loss, AUC:	0.0347,	0.932

Epoch 500: train loss, AUC:	0.0220,	0.944
Epoch 500: valid loss, AUC:	0.0346,	0.928
End of training cycles
Best train loss:	2.186e-02, best train AUC:	0.9455
Best valid epoch: 72
Best valid loss :	3.258e-02, best valid AUC:	0.9362
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None/checkpoint_best_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4501.8 MiB   4501.8 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4501.8 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4501.8 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4501.8 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4501.8 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4501.8 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4511.4 MiB     -0.5 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4511.4 MiB      9.0 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4511.4 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4511.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4511.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4511.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4511.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   4557.1 MiB  -8432.6 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   5041.3 MiB  -6283.5 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   4557.1 MiB -244947.3 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   4557.1 MiB  -8436.0 MiB         500           train_metrics.append(train_metric)
   225   4557.1 MiB  -8436.0 MiB         500           valid_metrics.append(valid_metric)
   226   4557.1 MiB  -8436.0 MiB         500           train_losses.append(train_loss)
   227   4557.1 MiB  -8436.0 MiB         500           valid_losses.append(valid_loss)
   228   4557.1 MiB  -8436.0 MiB         500           if e % (n_epochs // 10) == 0:
   229   4556.8 MiB   -165.0 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   230   4556.8 MiB   -163.1 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   231                                         
   232                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   233                                                 # performance for whatever reasons
   234   4557.1 MiB  -8434.1 MiB         500           if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc)\
   235   4557.1 MiB  -8433.8 MiB         491                         or valid_metric['auc'] > best_val_auc):
   236   4536.2 MiB     -2.2 MiB           9               best_epoch = e
   237   4536.2 MiB     -0.9 MiB           9               best_val_loss = valid_loss
   238   4536.2 MiB     -0.9 MiB           9               best_val_auc = valid_metric['auc']
   239   4536.2 MiB     -0.9 MiB           9               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   240                                         
   241   4554.6 MiB     -2.5 MiB           1       print(f'End of training cycles')
   242   4554.6 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   243   4554.6 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   244   4554.6 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   245   4554.6 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   246   4554.8 MiB      0.2 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   247   4554.8 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None/train_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None/valid_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None/train_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None/valid_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_all_KFold_3_231125_1402_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 98 minutes, 58 seconds.
Iteration 3 completed
Running iteration 4
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    256.4 MiB    256.4 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    256.4 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    256.4 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3851.1 MiB   3594.7 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3851.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3851.1 MiB      0.0 MiB           1       if return_dataset:
   224   3851.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3851.1 MiB   3851.1 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3851.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3851.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4462.7 MiB    611.6 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4462.7 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4462.7 MiB      0.0 MiB           1       if return_dataset:
   224   4462.7 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4462.7 MiB   4462.7 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4462.7 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4462.7 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4462.7 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4462.7 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4462.7 MiB      0.0 MiB           1       if return_dataset:
   224   4462.7 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0264,	0.932
Epoch 50: valid loss, AUC:	0.0389,	0.900

Epoch 100: train loss, AUC:	0.0246,	0.940
Epoch 100: valid loss, AUC:	0.0374,	0.886

Epoch 150: train loss, AUC:	0.0233,	0.943
Epoch 150: valid loss, AUC:	0.0384,	0.887

Epoch 200: train loss, AUC:	0.0227,	0.944
Epoch 200: valid loss, AUC:	0.0396,	0.890

Epoch 250: train loss, AUC:	0.0222,	0.946
Epoch 250: valid loss, AUC:	0.0389,	0.887

Epoch 300: train loss, AUC:	0.0218,	0.949
Epoch 300: valid loss, AUC:	0.0393,	0.883

Epoch 350: train loss, AUC:	0.0215,	0.949
Epoch 350: valid loss, AUC:	0.0409,	0.897

Epoch 400: train loss, AUC:	0.0213,	0.947
Epoch 400: valid loss, AUC:	0.0393,	0.890

Epoch 450: train loss, AUC:	0.0211,	0.951
Epoch 450: valid loss, AUC:	0.0407,	0.900

Epoch 500: train loss, AUC:	0.0208,	0.951
Epoch 500: valid loss, AUC:	0.0398,	0.894
End of training cycles
Best train loss:	2.083e-02, best train AUC:	0.9516
Best valid epoch: 114
Best valid loss :	3.959e-02, best valid AUC:	0.9027
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None/checkpoint_best_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4462.7 MiB   4462.7 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4462.7 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4462.7 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4462.7 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4462.7 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4462.7 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   9132.7 MiB  -1599.6 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   9132.7 MiB   3070.4 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   8874.4 MiB   -258.3 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   8874.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   8874.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   8874.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   8874.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   8878.3 MiB   -546.9 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   8878.4 MiB   -746.0 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   8878.3 MiB   -147.0 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   8878.3 MiB   -549.4 MiB         500           train_metrics.append(train_metric)
   225   8878.3 MiB   -549.4 MiB         500           valid_metrics.append(valid_metric)
   226   8878.3 MiB   -549.4 MiB         500           train_losses.append(train_loss)
   227   8878.3 MiB   -549.4 MiB         500           valid_losses.append(valid_loss)
   228   8878.3 MiB   -549.4 MiB         500           if e % (n_epochs // 10) == 0:
   229   8877.6 MiB    -11.9 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   230   8877.6 MiB     -2.9 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   231                                         
   232                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   233                                                 # performance for whatever reasons
   234   8878.3 MiB   -540.4 MiB         500           if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc)\
   235   8878.3 MiB   -544.8 MiB         493                         or valid_metric['auc'] > best_val_auc):
   236   8876.1 MiB     -6.0 MiB           8               best_epoch = e
   237   8876.1 MiB     -3.8 MiB           8               best_val_loss = valid_loss
   238   8876.1 MiB     -3.8 MiB           8               best_val_auc = valid_metric['auc']
   239   8876.1 MiB     -3.8 MiB           8               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   240                                         
   241   8877.6 MiB     -0.7 MiB           1       print(f'End of training cycles')
   242   8877.6 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   243   8877.6 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   244   8877.6 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   245   8877.6 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   246   8877.7 MiB      0.1 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   247   8877.7 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None/train_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None/valid_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None/train_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None/valid_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_all_KFold_4_231125_1541_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 76 minutes, 26 seconds.
Iteration 4 completed
Running iteration 5
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    257.3 MiB    257.3 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    257.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    257.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3831.9 MiB   3574.6 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3831.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3831.9 MiB      0.0 MiB           1       if return_dataset:
   224   3831.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3831.9 MiB   3831.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3831.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3831.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4465.1 MiB    633.2 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4465.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4465.1 MiB      0.0 MiB           1       if return_dataset:
   224   4465.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4465.1 MiB   4465.1 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4465.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4465.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4465.1 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4465.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4465.1 MiB      0.0 MiB           1       if return_dataset:
   224   4465.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0268,	0.931
Epoch 50: valid loss, AUC:	0.0353,	0.920

Epoch 100: train loss, AUC:	0.0249,	0.935
Epoch 100: valid loss, AUC:	0.0360,	0.920

Epoch 150: train loss, AUC:	0.0240,	0.940
Epoch 150: valid loss, AUC:	0.0361,	0.915

Epoch 200: train loss, AUC:	0.0235,	0.941
Epoch 200: valid loss, AUC:	0.0376,	0.909

Epoch 250: train loss, AUC:	0.0228,	0.943
Epoch 250: valid loss, AUC:	0.0373,	0.911

Epoch 300: train loss, AUC:	0.0226,	0.943
Epoch 300: valid loss, AUC:	0.0372,	0.914

Epoch 350: train loss, AUC:	0.0222,	0.945
Epoch 350: valid loss, AUC:	0.0367,	0.909

Epoch 400: train loss, AUC:	0.0219,	0.944
Epoch 400: valid loss, AUC:	0.0371,	0.912

Epoch 450: train loss, AUC:	0.0216,	0.946
Epoch 450: valid loss, AUC:	0.0370,	0.910

Epoch 500: train loss, AUC:	0.0215,	0.947
Epoch 500: valid loss, AUC:	0.0377,	0.911
End of training cycles
Best train loss:	2.138e-02, best train AUC:	0.9482
Best valid epoch: 69
Best valid loss :	3.555e-02, best valid AUC:	0.9241
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None/checkpoint_best_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170   4465.1 MiB   4465.1 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198   4465.1 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205   4465.1 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207   4465.1 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208   4465.1 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209   4465.1 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210   4518.4 MiB     -9.0 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211   4518.4 MiB     44.3 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212   4518.4 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214   4518.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217   4518.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219   4518.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220   4518.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221   6077.5 MiB -507417.7 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222   7390.6 MiB -462009.9 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223   6077.5 MiB -1369852.4 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224   6077.5 MiB -508585.4 MiB         500           train_metrics.append(train_metric)
   225   6077.5 MiB -508585.4 MiB         500           valid_metrics.append(valid_metric)
   226   6077.5 MiB -508585.4 MiB         500           train_losses.append(train_loss)
   227   6077.5 MiB -508585.4 MiB         500           valid_losses.append(valid_loss)
   228   6077.5 MiB -508585.4 MiB         500           if e % (n_epochs // 10) == 0:
   229   4516.9 MiB -10944.0 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   230   4516.9 MiB   -102.0 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   231                                         
   232                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   233                                                 # performance for whatever reasons
   234   6077.5 MiB -497743.4 MiB         500           if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc)\
   235   6077.5 MiB -508081.9 MiB         493                         or valid_metric['auc'] > best_val_auc):
   236   4473.7 MiB  -1174.6 MiB          10               best_epoch = e
   237   4473.7 MiB     -7.0 MiB          10               best_val_loss = valid_loss
   238   4473.7 MiB     -7.0 MiB          10               best_val_auc = valid_metric['auc']
   239   4473.7 MiB     -7.0 MiB          10               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   240                                         
   241   4501.1 MiB  -1576.4 MiB           1       print(f'End of training cycles')
   242   4501.1 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   243   4501.1 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   244   4501.1 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   245   4501.1 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   246   4501.1 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   247   4501.1 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None/train_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None/valid_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None/train_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None/valid_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_all_KFold_5_231125_1657_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 96 minutes, 34 seconds.
Iteration 5 completed
Script finished
