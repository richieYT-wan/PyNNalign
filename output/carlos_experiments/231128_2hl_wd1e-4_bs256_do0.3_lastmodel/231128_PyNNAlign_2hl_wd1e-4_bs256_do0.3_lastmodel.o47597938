INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld
+LD_GOLD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CPP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cpp
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC_NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
INFO: activate-gfortran_linux-64.sh made the following environmental changes:
+DEBUG_FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+DEBUG_FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+F77=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+F95=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-f95
+FC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+GFORTRAN=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-g++
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/pyscripts
"Starting PyScript"
Running iteration 1
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    256.8 MiB    256.8 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    256.8 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    256.8 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3736.9 MiB   3480.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3736.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3736.9 MiB      0.0 MiB           1       if return_dataset:
   224   3736.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3736.9 MiB   3736.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3736.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3736.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4444.6 MiB    707.7 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4444.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4444.6 MiB      0.0 MiB           1       if return_dataset:
   224   4444.6 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4444.6 MiB   4444.6 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4444.6 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4444.6 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4444.6 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4444.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4444.6 MiB      0.0 MiB           1       if return_dataset:
   224   4444.6 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0333,	0.914
Epoch 50: valid loss, AUC:	0.0368,	0.870

Epoch 50: train loss, AUC:	0.0333,	0.914
Epoch 50: valid loss, AUC:	0.0368,	0.870

Epoch 100: train loss, AUC:	0.0322,	0.919
Epoch 100: valid loss, AUC:	0.0354,	0.869

Epoch 100: train loss, AUC:	0.0322,	0.919
Epoch 100: valid loss, AUC:	0.0354,	0.869

Epoch 150: train loss, AUC:	0.0315,	0.919
Epoch 150: valid loss, AUC:	0.0357,	0.874

Epoch 150: train loss, AUC:	0.0315,	0.919
Epoch 150: valid loss, AUC:	0.0357,	0.874

Epoch 200: train loss, AUC:	0.0310,	0.921
Epoch 200: valid loss, AUC:	0.0372,	0.878

Epoch 200: train loss, AUC:	0.0310,	0.921
Epoch 200: valid loss, AUC:	0.0372,	0.878

Epoch 250: train loss, AUC:	0.0306,	0.922
Epoch 250: valid loss, AUC:	0.0357,	0.875

Epoch 250: train loss, AUC:	0.0306,	0.922
Epoch 250: valid loss, AUC:	0.0357,	0.875

Epoch 300: train loss, AUC:	0.0303,	0.925
Epoch 300: valid loss, AUC:	0.0363,	0.872

Epoch 300: train loss, AUC:	0.0303,	0.925
Epoch 300: valid loss, AUC:	0.0363,	0.872

Epoch 350: train loss, AUC:	0.0303,	0.928
Epoch 350: valid loss, AUC:	0.0352,	0.866

Epoch 350: train loss, AUC:	0.0303,	0.928
Epoch 350: valid loss, AUC:	0.0352,	0.866

Epoch 400: train loss, AUC:	0.0300,	0.928
Epoch 400: valid loss, AUC:	0.0358,	0.868

Epoch 400: train loss, AUC:	0.0300,	0.928
Epoch 400: valid loss, AUC:	0.0358,	0.868

Epoch 450: train loss, AUC:	0.0300,	0.925
Epoch 450: valid loss, AUC:	0.0358,	0.861

Epoch 450: train loss, AUC:	0.0300,	0.925
Epoch 450: valid loss, AUC:	0.0358,	0.861

Epoch 500: train loss, AUC:	0.0299,	0.929
Epoch 500: valid loss, AUC:	0.0365,	0.863

Epoch 500: train loss, AUC:	0.0299,	0.929
Epoch 500: valid loss, AUC:	0.0365,	0.863
End of training cycles
Best train loss:	2.963e-02, best train AUC:	0.9302
Best valid epoch: 500
Best valid loss :	3.653e-02, best valid AUC:	0.8631
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None/checkpoint_best_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4444.6 MiB   4444.6 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4444.6 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4444.6 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4444.6 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4444.6 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4444.6 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   4478.7 MiB     -1.0 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   4478.7 MiB     33.1 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   4478.7 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   4478.7 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   4478.8 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   4478.8 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   4478.8 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   4499.6 MiB -11283.5 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   4650.7 MiB -10924.4 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   4499.6 MiB -49755.4 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   4499.6 MiB -11327.8 MiB         500           train_metrics.append(train_metric)
   226   4499.6 MiB -11327.8 MiB         500           valid_metrics.append(valid_metric)
   227   4499.6 MiB -11327.8 MiB         500           train_losses.append(train_loss)
   228   4499.6 MiB -11327.8 MiB         500           valid_losses.append(valid_loss)
   229   4499.6 MiB -11327.8 MiB         500           if e % (n_epochs // 10) == 0:
   230   4499.5 MiB   -225.1 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231   4499.5 MiB   -223.7 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   4499.5 MiB   -223.7 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   4499.5 MiB   -223.7 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   4499.6 MiB -11326.4 MiB         500           if e == n_epochs:
   240   4455.3 MiB    -44.3 MiB           1               best_epoch = e
   241   4455.3 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   4455.3 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   4455.3 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   4455.3 MiB    -44.3 MiB           1       print(f'End of training cycles')
   246   4455.3 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   4455.3 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   4455.3 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   4455.3 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   4455.3 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   4455.3 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None/train_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None/valid_losses_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None/train_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None/valid_metrics_kcv_train_all5_correct_f01_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_1_231128_2046_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 85 minutes, 15 seconds.
Iteration 1 completed
Running iteration 2
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    256.5 MiB    256.5 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    256.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    256.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3657.3 MiB   3400.8 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3657.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3657.3 MiB      0.0 MiB           1       if return_dataset:
   224   3657.3 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3657.3 MiB   3657.3 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3657.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3657.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4504.9 MiB    847.6 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4504.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4504.9 MiB      0.0 MiB           1       if return_dataset:
   224   4504.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4504.9 MiB   4504.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4504.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4504.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4504.9 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4504.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4504.9 MiB      0.0 MiB           1       if return_dataset:
   224   4504.9 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0332,	0.910
Epoch 50: valid loss, AUC:	0.0373,	0.885

Epoch 50: train loss, AUC:	0.0332,	0.910
Epoch 50: valid loss, AUC:	0.0373,	0.885

Epoch 100: train loss, AUC:	0.0317,	0.916
Epoch 100: valid loss, AUC:	0.0360,	0.891

Epoch 100: train loss, AUC:	0.0317,	0.916
Epoch 100: valid loss, AUC:	0.0360,	0.891

Epoch 150: train loss, AUC:	0.0310,	0.916
Epoch 150: valid loss, AUC:	0.0346,	0.894

Epoch 150: train loss, AUC:	0.0310,	0.916
Epoch 150: valid loss, AUC:	0.0346,	0.894

Epoch 200: train loss, AUC:	0.0306,	0.919
Epoch 200: valid loss, AUC:	0.0356,	0.887

Epoch 200: train loss, AUC:	0.0306,	0.919
Epoch 200: valid loss, AUC:	0.0356,	0.887

Epoch 250: train loss, AUC:	0.0302,	0.916
Epoch 250: valid loss, AUC:	0.0358,	0.894

Epoch 250: train loss, AUC:	0.0302,	0.916
Epoch 250: valid loss, AUC:	0.0358,	0.894

Epoch 300: train loss, AUC:	0.0300,	0.923
Epoch 300: valid loss, AUC:	0.0361,	0.890

Epoch 300: train loss, AUC:	0.0300,	0.923
Epoch 300: valid loss, AUC:	0.0361,	0.890

Epoch 350: train loss, AUC:	0.0298,	0.921
Epoch 350: valid loss, AUC:	0.0370,	0.895

Epoch 350: train loss, AUC:	0.0298,	0.921
Epoch 350: valid loss, AUC:	0.0370,	0.895

Epoch 400: train loss, AUC:	0.0295,	0.925
Epoch 400: valid loss, AUC:	0.0368,	0.890

Epoch 400: train loss, AUC:	0.0295,	0.925
Epoch 400: valid loss, AUC:	0.0368,	0.890

Epoch 450: train loss, AUC:	0.0295,	0.924
Epoch 450: valid loss, AUC:	0.0371,	0.884

Epoch 450: train loss, AUC:	0.0295,	0.924
Epoch 450: valid loss, AUC:	0.0371,	0.884

Epoch 500: train loss, AUC:	0.0295,	0.922
Epoch 500: valid loss, AUC:	0.0379,	0.889

Epoch 500: train loss, AUC:	0.0295,	0.922
Epoch 500: valid loss, AUC:	0.0379,	0.889
End of training cycles
Best train loss:	2.925e-02, best train AUC:	0.927
Best valid epoch: 500
Best valid loss :	3.787e-02, best valid AUC:	0.8895
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None/checkpoint_best_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4504.9 MiB   4504.9 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4504.9 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4504.9 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4504.9 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4504.9 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4504.9 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   4512.4 MiB     -0.1 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   4512.4 MiB      7.2 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   4512.4 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   4512.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   4512.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   4512.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   4512.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   4514.4 MiB   -141.8 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   4515.3 MiB    115.0 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   4514.4 MiB   -459.9 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   4514.4 MiB   -142.1 MiB         500           train_metrics.append(train_metric)
   226   4514.4 MiB   -142.1 MiB         500           valid_metrics.append(valid_metric)
   227   4514.4 MiB   -142.1 MiB         500           train_losses.append(train_loss)
   228   4514.4 MiB   -142.1 MiB         500           valid_losses.append(valid_loss)
   229   4514.4 MiB   -142.1 MiB         500           if e % (n_epochs // 10) == 0:
   230   4514.2 MiB     -2.8 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231   4514.2 MiB     -0.2 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   4514.2 MiB     -0.2 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   4514.2 MiB     -0.2 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   4514.4 MiB   -139.5 MiB         500           if e == n_epochs:
   240   4514.2 MiB     -0.2 MiB           1               best_epoch = e
   241   4514.2 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   4514.2 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   4514.2 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   4514.2 MiB     -0.2 MiB           1       print(f'End of training cycles')
   246   4514.2 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   4514.2 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   4514.2 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   4514.2 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   4514.4 MiB      0.2 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   4514.4 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None/train_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None/valid_losses_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None/train_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None/valid_metrics_kcv_train_all5_correct_f02_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_2_231128_2212_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 91 minutes, 31 seconds.
Iteration 2 completed
Running iteration 3
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    257.4 MiB    257.4 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    257.4 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    257.4 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3673.3 MiB   3415.9 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3673.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3673.3 MiB      0.0 MiB           1       if return_dataset:
   224   3673.3 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3673.3 MiB   3673.3 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3673.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3673.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4506.1 MiB    832.8 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4506.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4506.1 MiB      0.0 MiB           1       if return_dataset:
   224   4506.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4506.1 MiB   4506.1 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4506.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4506.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4506.1 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4506.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4506.1 MiB      0.0 MiB           1       if return_dataset:
   224   4506.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0335,	0.896
Epoch 50: valid loss, AUC:	0.0345,	0.927

Epoch 50: train loss, AUC:	0.0335,	0.896
Epoch 50: valid loss, AUC:	0.0345,	0.927

Epoch 100: train loss, AUC:	0.0321,	0.903
Epoch 100: valid loss, AUC:	0.0348,	0.929

Epoch 100: train loss, AUC:	0.0321,	0.903
Epoch 100: valid loss, AUC:	0.0348,	0.929

Epoch 150: train loss, AUC:	0.0314,	0.907
Epoch 150: valid loss, AUC:	0.0381,	0.917

Epoch 150: train loss, AUC:	0.0314,	0.907
Epoch 150: valid loss, AUC:	0.0381,	0.917

Epoch 200: train loss, AUC:	0.0311,	0.907
Epoch 200: valid loss, AUC:	0.0359,	0.929

Epoch 200: train loss, AUC:	0.0311,	0.907
Epoch 200: valid loss, AUC:	0.0359,	0.929

Epoch 250: train loss, AUC:	0.0307,	0.911
Epoch 250: valid loss, AUC:	0.0364,	0.921

Epoch 250: train loss, AUC:	0.0307,	0.911
Epoch 250: valid loss, AUC:	0.0364,	0.921

Epoch 300: train loss, AUC:	0.0303,	0.914
Epoch 300: valid loss, AUC:	0.0341,	0.927

Epoch 300: train loss, AUC:	0.0303,	0.914
Epoch 300: valid loss, AUC:	0.0341,	0.927

Epoch 350: train loss, AUC:	0.0302,	0.915
Epoch 350: valid loss, AUC:	0.0356,	0.922

Epoch 350: train loss, AUC:	0.0302,	0.915
Epoch 350: valid loss, AUC:	0.0356,	0.922

Epoch 400: train loss, AUC:	0.0300,	0.914
Epoch 400: valid loss, AUC:	0.0347,	0.925

Epoch 400: train loss, AUC:	0.0300,	0.914
Epoch 400: valid loss, AUC:	0.0347,	0.925

Epoch 450: train loss, AUC:	0.0300,	0.910
Epoch 450: valid loss, AUC:	0.0362,	0.922

Epoch 450: train loss, AUC:	0.0300,	0.910
Epoch 450: valid loss, AUC:	0.0362,	0.922

Epoch 500: train loss, AUC:	0.0299,	0.914
Epoch 500: valid loss, AUC:	0.0350,	0.924

Epoch 500: train loss, AUC:	0.0299,	0.914
Epoch 500: valid loss, AUC:	0.0350,	0.924
End of training cycles
Best train loss:	2.977e-02, best train AUC:	0.9218
Best valid epoch: 500
Best valid loss :	3.502e-02, best valid AUC:	0.9244
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None/checkpoint_best_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4506.1 MiB   4506.1 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4506.1 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4506.1 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4506.1 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4506.1 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4506.1 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   6964.3 MiB     -0.4 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   6964.3 MiB   2457.7 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   6964.3 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   6964.3 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   6964.4 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   6964.4 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   6964.4 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   6966.5 MiB   -130.6 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   6967.6 MiB    258.3 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   6966.5 MiB   -679.9 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   6966.5 MiB   -130.9 MiB         500           train_metrics.append(train_metric)
   226   6966.5 MiB   -130.9 MiB         500           valid_metrics.append(valid_metric)
   227   6966.5 MiB   -130.9 MiB         500           train_losses.append(train_loss)
   228   6966.5 MiB   -130.9 MiB         500           valid_losses.append(valid_loss)
   229   6966.5 MiB   -130.9 MiB         500           if e % (n_epochs // 10) == 0:
   230   6966.3 MiB     -2.6 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231   6966.3 MiB     -0.3 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   6966.3 MiB     -0.3 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   6966.3 MiB     -0.3 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   6966.5 MiB   -128.6 MiB         500           if e == n_epochs:
   240   6966.3 MiB     -0.2 MiB           1               best_epoch = e
   241   6966.3 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   6966.3 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   6966.3 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   6966.3 MiB     -0.2 MiB           1       print(f'End of training cycles')
   246   6966.3 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   6966.3 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   6966.3 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   6966.3 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   6966.5 MiB      0.2 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   6966.5 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None/train_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None/valid_losses_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None/train_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None/valid_metrics_kcv_train_all5_correct_f03_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_3_231128_2344_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 76 minutes, 10 seconds.
Iteration 3 completed
Running iteration 4
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    257.6 MiB    257.6 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    257.6 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    257.6 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3851.3 MiB   3593.7 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3851.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3851.3 MiB      0.0 MiB           1       if return_dataset:
   224   3851.3 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3851.3 MiB   3851.3 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3851.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3851.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4466.5 MiB    615.2 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4466.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4466.5 MiB      0.0 MiB           1       if return_dataset:
   224   4466.5 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4466.5 MiB   4466.5 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4466.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4466.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4466.5 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4466.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4466.5 MiB      0.0 MiB           1       if return_dataset:
   224   4466.5 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0326,	0.911
Epoch 50: valid loss, AUC:	0.0381,	0.883

Epoch 50: train loss, AUC:	0.0326,	0.911
Epoch 50: valid loss, AUC:	0.0381,	0.883

Epoch 100: train loss, AUC:	0.0312,	0.918
Epoch 100: valid loss, AUC:	0.0383,	0.886

Epoch 100: train loss, AUC:	0.0312,	0.918
Epoch 100: valid loss, AUC:	0.0383,	0.886

Epoch 150: train loss, AUC:	0.0305,	0.918
Epoch 150: valid loss, AUC:	0.0375,	0.883

Epoch 150: train loss, AUC:	0.0305,	0.918
Epoch 150: valid loss, AUC:	0.0375,	0.883

Epoch 200: train loss, AUC:	0.0300,	0.921
Epoch 200: valid loss, AUC:	0.0376,	0.882

Epoch 200: train loss, AUC:	0.0300,	0.921
Epoch 200: valid loss, AUC:	0.0376,	0.882

Epoch 250: train loss, AUC:	0.0297,	0.923
Epoch 250: valid loss, AUC:	0.0381,	0.884

Epoch 250: train loss, AUC:	0.0297,	0.923
Epoch 250: valid loss, AUC:	0.0381,	0.884

Epoch 300: train loss, AUC:	0.0295,	0.924
Epoch 300: valid loss, AUC:	0.0370,	0.887

Epoch 300: train loss, AUC:	0.0295,	0.924
Epoch 300: valid loss, AUC:	0.0370,	0.887

Epoch 350: train loss, AUC:	0.0293,	0.924
Epoch 350: valid loss, AUC:	0.0376,	0.875

Epoch 350: train loss, AUC:	0.0293,	0.924
Epoch 350: valid loss, AUC:	0.0376,	0.875

Epoch 400: train loss, AUC:	0.0291,	0.923
Epoch 400: valid loss, AUC:	0.0380,	0.880

Epoch 400: train loss, AUC:	0.0291,	0.923
Epoch 400: valid loss, AUC:	0.0380,	0.880

Epoch 450: train loss, AUC:	0.0291,	0.927
Epoch 450: valid loss, AUC:	0.0378,	0.875

Epoch 450: train loss, AUC:	0.0291,	0.927
Epoch 450: valid loss, AUC:	0.0378,	0.875

Epoch 500: train loss, AUC:	0.0289,	0.929
Epoch 500: valid loss, AUC:	0.0376,	0.881

Epoch 500: train loss, AUC:	0.0289,	0.929
Epoch 500: valid loss, AUC:	0.0376,	0.881
End of training cycles
Best train loss:	2.881e-02, best train AUC:	0.9295
Best valid epoch: 500
Best valid loss :	3.763e-02, best valid AUC:	0.8808
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None/checkpoint_best_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4466.5 MiB   4466.5 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4466.5 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4466.5 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4466.5 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4466.5 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4466.5 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   4970.5 MiB  -1389.4 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   4970.5 MiB   -885.5 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   4816.4 MiB   -154.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   4816.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   4816.5 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   4816.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   4816.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   5911.3 MiB -159909.5 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   5911.6 MiB -158308.4 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   5911.3 MiB -255658.2 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   5911.3 MiB -160974.6 MiB         500           train_metrics.append(train_metric)
   226   5911.3 MiB -160974.6 MiB         500           valid_metrics.append(valid_metric)
   227   5911.3 MiB -160974.6 MiB         500           train_losses.append(train_loss)
   228   5911.3 MiB -160974.6 MiB         500           valid_losses.append(valid_loss)
   229   5911.3 MiB -160974.6 MiB         500           if e % (n_epochs // 10) == 0:
   230   5062.2 MiB  -3827.7 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231   5062.2 MiB  -2115.4 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   5062.2 MiB  -2115.4 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   5062.2 MiB  -2115.4 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   5911.3 MiB -159262.3 MiB         500           if e == n_epochs:
   240   4846.2 MiB  -1065.0 MiB           1               best_epoch = e
   241   4846.2 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   4846.2 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   4846.2 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   4846.2 MiB  -1065.0 MiB           1       print(f'End of training cycles')
   246   4846.2 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   4846.2 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   4846.2 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   4846.2 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   4846.3 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   4846.3 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None/train_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None/valid_losses_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None/train_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None/valid_metrics_kcv_train_all5_correct_f04_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_4_231129_0100_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 75 minutes, 57 seconds.
Iteration 4 completed
Running iteration 5
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    258.7 MiB    258.7 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    258.7 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    258.7 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   3831.6 MiB   3572.9 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   3831.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   3831.6 MiB      0.0 MiB           1       if return_dataset:
   224   3831.6 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   3831.6 MiB   3831.6 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   3831.6 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   3831.6 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4464.3 MiB    632.6 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4464.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4464.3 MiB      0.0 MiB           1       if return_dataset:
   224   4464.3 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   4464.3 MiB   4464.3 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   4464.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   4464.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   4464.3 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   4464.3 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   4464.3 MiB      0.0 MiB           1       if return_dataset:
   224   4464.3 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0326,	0.909
Epoch 50: valid loss, AUC:	0.0369,	0.918

Epoch 50: train loss, AUC:	0.0326,	0.909
Epoch 50: valid loss, AUC:	0.0369,	0.918

Epoch 100: train loss, AUC:	0.0311,	0.911
Epoch 100: valid loss, AUC:	0.0359,	0.918

Epoch 100: train loss, AUC:	0.0311,	0.911
Epoch 100: valid loss, AUC:	0.0359,	0.918

Epoch 150: train loss, AUC:	0.0305,	0.915
Epoch 150: valid loss, AUC:	0.0356,	0.922

Epoch 150: train loss, AUC:	0.0305,	0.915
Epoch 150: valid loss, AUC:	0.0356,	0.922

Epoch 200: train loss, AUC:	0.0300,	0.920
Epoch 200: valid loss, AUC:	0.0360,	0.916

Epoch 200: train loss, AUC:	0.0300,	0.920
Epoch 200: valid loss, AUC:	0.0360,	0.916

Epoch 250: train loss, AUC:	0.0297,	0.916
Epoch 250: valid loss, AUC:	0.0358,	0.923

Epoch 250: train loss, AUC:	0.0297,	0.916
Epoch 250: valid loss, AUC:	0.0358,	0.923

Epoch 300: train loss, AUC:	0.0296,	0.921
Epoch 300: valid loss, AUC:	0.0353,	0.922

Epoch 300: train loss, AUC:	0.0296,	0.921
Epoch 300: valid loss, AUC:	0.0353,	0.922

Epoch 350: train loss, AUC:	0.0294,	0.917
Epoch 350: valid loss, AUC:	0.0359,	0.919

Epoch 350: train loss, AUC:	0.0294,	0.917
Epoch 350: valid loss, AUC:	0.0359,	0.919

Epoch 400: train loss, AUC:	0.0294,	0.921
Epoch 400: valid loss, AUC:	0.0355,	0.923

Epoch 400: train loss, AUC:	0.0294,	0.921
Epoch 400: valid loss, AUC:	0.0355,	0.923

Epoch 450: train loss, AUC:	0.0292,	0.919
Epoch 450: valid loss, AUC:	0.0360,	0.926

Epoch 450: train loss, AUC:	0.0292,	0.919
Epoch 450: valid loss, AUC:	0.0360,	0.926

Epoch 500: train loss, AUC:	0.0290,	0.921
Epoch 500: valid loss, AUC:	0.0360,	0.927

Epoch 500: train loss, AUC:	0.0290,	0.921
Epoch 500: valid loss, AUC:	0.0360,	0.927
End of training cycles
Best train loss:	2.897e-02, best train AUC:	0.9244
Best valid epoch: 500
Best valid loss :	3.604e-02, best valid AUC:	0.9271
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None/checkpoint_best_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171   4464.3 MiB   4464.3 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199   4464.3 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206   4464.3 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208   4464.3 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209   4464.3 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210   4464.3 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211   4511.5 MiB     -2.0 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212   4511.5 MiB     45.1 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213   4511.5 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   214                                         
   215   4511.5 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218   4511.5 MiB      0.0 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220   4511.5 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221   4511.5 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222   5921.5 MiB -621260.0 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223   6389.2 MiB -610901.6 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224   5921.5 MiB -751076.2 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225   5921.5 MiB -622706.6 MiB         500           train_metrics.append(train_metric)
   226   5921.5 MiB -622706.6 MiB         500           valid_metrics.append(valid_metric)
   227   5921.5 MiB -622706.6 MiB         500           train_losses.append(train_loss)
   228   5921.5 MiB -622706.6 MiB         500           valid_losses.append(valid_loss)
   229   5921.5 MiB -622706.6 MiB         500           if e % (n_epochs // 10) == 0:
   230   4500.5 MiB -13819.2 MiB          10               tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231   4500.5 MiB   -126.8 MiB          10               tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232   4500.5 MiB   -126.8 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233   4500.5 MiB   -126.8 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239   5921.5 MiB -609014.2 MiB         500           if e == n_epochs:
   240   4475.0 MiB  -1446.5 MiB           1               best_epoch = e
   241   4475.0 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242   4475.0 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243   4475.0 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245   4475.0 MiB  -1446.5 MiB           1       print(f'End of training cycles')
   246   4475.0 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247   4475.0 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248   4475.0 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249   4475.0 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250   4475.0 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251   4475.0 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None/train_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None/valid_losses_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None/train_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None/valid_metrics_kcv_train_all5_correct_f05_CSL_hp_2hl_wd1e-4_bs256_do0.3_lastmodel_KFold_5_231129_0216_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 69 minutes, 30 seconds.
Iteration 5 completed
Script finished
