INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld
+LD_GOLD=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CPP=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-cpp
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_AR=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC_NM=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
INFO: activate-gfortran_linux-64.sh made the following environmental changes:
+DEBUG_FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+DEBUG_FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -pipe
+F77=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+F95=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-f95
+FC=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
+FFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+FORTRANFLAGS=-fopenmp -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+GFORTRAN=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-gfortran
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-c++
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/services/tools/anaconda3/4.4.0/bin/x86_64-conda_cos6-linux-gnu-g++
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/pyscripts
"Starting PyScript"
Running iteration 1
torch.Size([106949, 37, 20])
torch.Size([27332, 37, 20])
torch.Size([17, 37, 20])
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0394,	0.893
Epoch 50: valid loss, AUC:	0.0420,	0.859

Epoch 100: train loss, AUC:	0.0393,	0.893
Epoch 100: valid loss, AUC:	0.0421,	0.863

Epoch 150: train loss, AUC:	0.0394,	0.893
Epoch 150: valid loss, AUC:	0.0423,	0.864

Epoch 200: train loss, AUC:	0.0393,	0.894
Epoch 200: valid loss, AUC:	0.0416,	0.858

Epoch 250: train loss, AUC:	0.0393,	0.893
Epoch 250: valid loss, AUC:	0.0417,	0.851

Epoch 300: train loss, AUC:	0.0392,	0.894
Epoch 300: valid loss, AUC:	0.0422,	0.863

Epoch 350: train loss, AUC:	0.0393,	0.893
Epoch 350: valid loss, AUC:	0.0417,	0.862

Epoch 400: train loss, AUC:	0.0393,	0.896
Epoch 400: valid loss, AUC:	0.0412,	0.858

Epoch 450: train loss, AUC:	0.0393,	0.892
Epoch 450: valid loss, AUC:	0.0429,	0.857

Epoch 500: train loss, AUC:	0.0392,	0.893
Epoch 500: valid loss, AUC:	0.0418,	0.859
Shape of trainset: (tensor([[ -2.,  -2.,   2.,  ...,  15.,   2.,  -3.],
        [ -2.,  -3.,  -4.,  ...,  -3.,   2.,  -4.],
        [ -2.,  -2.,   2.,  ...,  -2.,  -1.,   1.],
        ...,
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	3.918e-02, best train AUC:	0.8964
Best valid epoch: 347
Best valid loss :	4.249e-02, best valid AUC:	0.8769
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None/checkpoint_best_kcv_train_all5_correct_f01_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None/train_losses_kcv_train_all5_correct_f01_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None/valid_losses_kcv_train_all5_correct_f01_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None/train_metrics_kcv_train_all5_correct_f01_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None/valid_metrics_kcv_train_all5_correct_f01_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_1_231018_1158_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 125 minutes, 20 seconds.
Iteration 1 completed
Running iteration 2
torch.Size([104523, 37, 20])
torch.Size([29758, 37, 20])
torch.Size([17, 37, 20])
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0398,	0.887
Epoch 50: valid loss, AUC:	0.0424,	0.876

Epoch 100: train loss, AUC:	0.0396,	0.884
Epoch 100: valid loss, AUC:	0.0421,	0.870

Epoch 150: train loss, AUC:	0.0396,	0.885
Epoch 150: valid loss, AUC:	0.0424,	0.862

Epoch 200: train loss, AUC:	0.0397,	0.883
Epoch 200: valid loss, AUC:	0.0436,	0.867

Epoch 250: train loss, AUC:	0.0396,	0.885
Epoch 250: valid loss, AUC:	0.0428,	0.860

Epoch 300: train loss, AUC:	0.0395,	0.882
Epoch 300: valid loss, AUC:	0.0435,	0.867

Epoch 350: train loss, AUC:	0.0396,	0.886
Epoch 350: valid loss, AUC:	0.0427,	0.862

Epoch 400: train loss, AUC:	0.0396,	0.886
Epoch 400: valid loss, AUC:	0.0427,	0.869

Epoch 450: train loss, AUC:	0.0396,	0.885
Epoch 450: valid loss, AUC:	0.0435,	0.862

Epoch 500: train loss, AUC:	0.0397,	0.884
Epoch 500: valid loss, AUC:	0.0435,	0.855
Shape of trainset: (tensor([[ -1.,  -3.,  -2.,  ...,  -2.,  -1.,   1.],
        [ -1.,   3.,   0.,  ...,  -3.,  -2.,  -3.],
        [ -2.,  -1.,  -2.,  ...,  -2.,  -1.,   1.],
        ...,
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	3.945e-02, best train AUC:	0.8883
Best valid epoch: 50
Best valid loss :	4.240e-02, best valid AUC:	0.8758
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None/checkpoint_best_kcv_train_all5_correct_f02_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None/train_losses_kcv_train_all5_correct_f02_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None/valid_losses_kcv_train_all5_correct_f02_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None/train_metrics_kcv_train_all5_correct_f02_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None/valid_metrics_kcv_train_all5_correct_f02_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_2_231018_1403_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 67 minutes, 2 seconds.
Iteration 2 completed
Running iteration 3
torch.Size([105047, 37, 20])
torch.Size([29234, 37, 20])
torch.Size([17, 37, 20])
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0399,	0.875
Epoch 50: valid loss, AUC:	0.0401,	0.911

Epoch 100: train loss, AUC:	0.0398,	0.872
Epoch 100: valid loss, AUC:	0.0407,	0.912

Epoch 150: train loss, AUC:	0.0399,	0.874
Epoch 150: valid loss, AUC:	0.0399,	0.914

Epoch 200: train loss, AUC:	0.0397,	0.873
Epoch 200: valid loss, AUC:	0.0402,	0.915

Epoch 250: train loss, AUC:	0.0398,	0.872
Epoch 250: valid loss, AUC:	0.0419,	0.912

Epoch 300: train loss, AUC:	0.0397,	0.871
Epoch 300: valid loss, AUC:	0.0414,	0.911

Epoch 350: train loss, AUC:	0.0397,	0.872
Epoch 350: valid loss, AUC:	0.0409,	0.917

Epoch 400: train loss, AUC:	0.0398,	0.875
Epoch 400: valid loss, AUC:	0.0406,	0.914

Epoch 450: train loss, AUC:	0.0398,	0.870
Epoch 450: valid loss, AUC:	0.0398,	0.910

Epoch 500: train loss, AUC:	0.0397,	0.873
Epoch 500: valid loss, AUC:	0.0409,	0.913
Shape of trainset: (tensor([[ -1.,  -3.,  -2.,  ...,  -2.,  -1.,   1.],
        [ -1.,   3.,   0.,  ...,  -3.,  -2.,  -3.],
        [ -2.,  -1.,  -2.,  ...,  -2.,  -1.,   1.],
        ...,
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	3.959e-02, best train AUC:	0.8764
Best valid epoch: 110
Best valid loss :	4.078e-02, best valid AUC:	0.9183
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None/checkpoint_best_kcv_train_all5_correct_f03_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None/train_losses_kcv_train_all5_correct_f03_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None/valid_losses_kcv_train_all5_correct_f03_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None/train_metrics_kcv_train_all5_correct_f03_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None/valid_metrics_kcv_train_all5_correct_f03_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_3_231018_1511_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 66 minutes, 32 seconds.
Iteration 3 completed
Running iteration 4
torch.Size([110618, 37, 20])
torch.Size([23663, 37, 20])
torch.Size([17, 37, 20])
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0390,	0.888
Epoch 50: valid loss, AUC:	0.0428,	0.872

Epoch 100: train loss, AUC:	0.0389,	0.890
Epoch 100: valid loss, AUC:	0.0435,	0.865

Epoch 150: train loss, AUC:	0.0389,	0.887
Epoch 150: valid loss, AUC:	0.0427,	0.875

Epoch 200: train loss, AUC:	0.0389,	0.889
Epoch 200: valid loss, AUC:	0.0435,	0.868

Epoch 250: train loss, AUC:	0.0389,	0.889
Epoch 250: valid loss, AUC:	0.0429,	0.865

Epoch 300: train loss, AUC:	0.0389,	0.890
Epoch 300: valid loss, AUC:	0.0444,	0.871

Epoch 350: train loss, AUC:	0.0389,	0.888
Epoch 350: valid loss, AUC:	0.0430,	0.862

Epoch 400: train loss, AUC:	0.0388,	0.889
Epoch 400: valid loss, AUC:	0.0431,	0.875

Epoch 450: train loss, AUC:	0.0389,	0.889
Epoch 450: valid loss, AUC:	0.0427,	0.872

Epoch 500: train loss, AUC:	0.0389,	0.888
Epoch 500: valid loss, AUC:	0.0428,	0.871
Shape of trainset: (tensor([[ -1.,  -3.,  -2.,  ...,  -2.,  -1.,   1.],
        [ -1.,   3.,   0.,  ...,  -3.,  -2.,  -3.],
        [ -2.,  -1.,  -2.,  ...,  -2.,  -1.,   1.],
        ...,
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	3.874e-02, best train AUC:	0.8921
Best valid epoch: 403
Best valid loss :	4.395e-02, best valid AUC:	0.8813
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None/checkpoint_best_kcv_train_all5_correct_f04_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None/train_losses_kcv_train_all5_correct_f04_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None/valid_losses_kcv_train_all5_correct_f04_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None/train_metrics_kcv_train_all5_correct_f04_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None/valid_metrics_kcv_train_all5_correct_f04_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_4_231018_1617_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 67 minutes, 40 seconds.
Iteration 4 completed
Running iteration 5
torch.Size([109987, 37, 20])
torch.Size([24294, 37, 20])
torch.Size([17, 37, 20])
No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0395,	0.878
Epoch 50: valid loss, AUC:	0.0424,	0.883

Epoch 100: train loss, AUC:	0.0393,	0.880
Epoch 100: valid loss, AUC:	0.0446,	0.888

Epoch 150: train loss, AUC:	0.0393,	0.880
Epoch 150: valid loss, AUC:	0.0423,	0.887

Epoch 200: train loss, AUC:	0.0393,	0.881
Epoch 200: valid loss, AUC:	0.0420,	0.887

Epoch 250: train loss, AUC:	0.0393,	0.879
Epoch 250: valid loss, AUC:	0.0422,	0.888

Epoch 300: train loss, AUC:	0.0392,	0.879
Epoch 300: valid loss, AUC:	0.0438,	0.890

Epoch 350: train loss, AUC:	0.0393,	0.881
Epoch 350: valid loss, AUC:	0.0426,	0.892

Epoch 400: train loss, AUC:	0.0393,	0.881
Epoch 400: valid loss, AUC:	0.0424,	0.889

Epoch 450: train loss, AUC:	0.0393,	0.881
Epoch 450: valid loss, AUC:	0.0423,	0.894

Epoch 500: train loss, AUC:	0.0393,	0.881
Epoch 500: valid loss, AUC:	0.0421,	0.888
Shape of trainset: (tensor([[ -1.,  -3.,  -2.,  ...,  -2.,  -1.,   1.],
        [ -1.,   3.,   0.,  ...,  -3.,  -2.,  -3.],
        [ -2.,  -1.,  -2.,  ...,  -2.,  -1.,   1.],
        ...,
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.],
        [-12., -12., -12.,  ..., -12., -12., -12.]]), tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]), tensor([-2., -1., -2., -3., -3., -1., -2., -3.,  2., -1., -1., -2.,  0.,  4.,
        -3., -2., -2.,  2.,  8., -1.,  5., -2., -1., -2., -1., -1., -1.,  0.,
        -2., -1., -2., -1., -1., -3., -1.,  1.,  0., -3., -2.,  0., -3., -3.,
        -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3.,
        -2.,  1.,  4., -1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,
         1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1., -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,
         0.,  8., -4., -3., -2.,  1.,  4., -1.,  1., -1.,  1.,  0., -1.,  0.,
        -1.,  0., -1., -3., -3.,  0., -2., -3., -1.,  5.,  2., -4., -2., -2.,
         0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4.,
        -2.,  0., -2., -3., -3., -4.,  0., -3.,  0., -1., -3., -2., -3.,  8.,
        -2., -4., -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4.,  5., -2.,
        -1., -2., -1., -1., -1.,  0., -2., -1., -2., -1., -1., -3., -1.,  1.,
         0., -3., -2.,  0., -1., -4., -3., -4., -2., -3., -4., -4., -4.,  5.,
         2., -3.,  2.,  0., -3., -3., -1., -3., -1.,  4., -2., -3., -4., -4.,
        -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3., -1., -2.,
        -1.,  1., -1., -1.,  7.,  2., -2.,  0.,  0.,  0.,  1., -3., -4.,  0.,
        -2., -4., -2.,  1.,  0., -4., -2., -3.,  0., -1.,  0., -1., -1., -1.,
        -1., -2., -2., -1., -1., -1., -1., -2., -1.,  2.,  5., -3., -2.,  0.,
        -2., -3., -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1.,
        -4., -3., -1., -2., -1.,  1., -3., -3., -4., -5., -2., -4., -3., -4.,
        -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,  4., -1.,  0., -3.,
         0., -1., -3., -2., -3.,  8., -2., -4., -4., -2., -3., -4., -2.,  0.,
        -2., -3., -3., -4., -1.,  1.,  0.,  0., -3.,  7.,  2., -2.,  1., -3.,
        -2.,  2.,  0., -4., -1.,  0., -1., -1., -1., -3., -3., -3., -4., -5.,
        -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8., -4., -3., -2.,  1.,
         4., -1., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4., -3.,  1.,
        -2., -3., -1., -1., -1., -3., -2., -3., -2., -1., -2., -3., -3., -1.,
        -2., -3.,  2., -1., -1., -2.,  0.,  4., -3., -2., -2.,  2.,  8., -1.,
        -3., -3., -4., -5., -2., -4., -3., -4., -1.,  0.,  1., -4.,  0.,  8.,
        -4., -3., -2.,  1.,  4., -1., -2., -2.,  2.,  8., -4.,  0.,  2., -1.,
        -1., -4., -4., -1., -4., -5., -1.,  0., -1., -5., -3., -4., -1., -4.,
        -3., -4., -2., -3., -4., -4., -4.,  5.,  2., -3.,  2.,  0., -3., -3.,
        -1., -3., -1.,  4., -1.,  0.,  0.,  2., -3.,  2.,  6., -3.,  0., -4.,
        -3.,  1., -2., -3., -1., -1., -1., -3., -2., -3., -1.,  0.,  0.,  2.,
        -3.,  2.,  6., -3.,  0., -4., -3.,  1., -2., -3., -1., -1., -1., -3.,
        -2., -3.,  0., -3., -3., -4., -1., -3., -3., -4., -4.,  4.,  1., -3.,
         1., -1., -3., -2.,  0., -3., -1.,  5., -2.,  7., -1., -2., -4.,  1.,
         0., -3.,  0., -4., -3.,  3., -2., -3., -3., -1., -1., -3., -1., -3.,
        -1., -2., -2., -4., -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0.,
        -3., -2., -1., -1.,  0.,  1., -2.,  0.,  1., -1., -3.,  1.,  0., -2.,
        10., -4., -3.,  0., -1., -1., -2., -1., -2., -3.,  2., -4., -2., -3.,
        -4., -4., -2., -2., -3., -4., -3.,  2.,  5., -3.,  3.,  1., -4., -3.,
        -1., -2., -1.,  1.,  0., -3.,  0., -1., -3., -2., -3.,  8., -2., -4.,
        -4., -2., -3., -4., -2.,  0., -2., -3., -3., -4., -1., -2., -2., -4.,
        -2.,  0., -2., -3., -1.,  2.,  3., -2.,  7.,  0., -3., -2., -1., -1.,
         0.,  1.,  0., -1.,  0., -1., -1., -1., -1., -2., -2., -1., -1., -1.,
        -1., -2., -1.,  2.,  5., -3., -2.,  0.]), tensor([0.]))
End of training cycles
Best train loss:	3.916e-02, best train AUC:	0.8834
Best valid epoch: 410
Best valid loss :	4.321e-02, best valid AUC:	0.8948
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None/checkpoint_best_kcv_train_all5_correct_f05_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None.pt
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None/train_losses_kcv_train_all5_correct_f05_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None/valid_losses_kcv_train_all5_correct_f05_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None/train_metrics_kcv_train_all5_correct_f05_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None/valid_metrics_kcv_train_all5_correct_f05_CSL_mhc_nobnorm_nostd_burn_60nh_all_KFold_5_231018_1725_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 142 minutes, 50 seconds.
Iteration 5 completed
Script finished
