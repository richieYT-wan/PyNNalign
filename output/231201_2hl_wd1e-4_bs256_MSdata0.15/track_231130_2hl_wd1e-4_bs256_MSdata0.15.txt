Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211    345.9 MiB    345.9 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218    345.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219    345.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220   9756.6 MiB   9410.7 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222   9756.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223   9756.6 MiB      0.0 MiB           1       if return_dataset:
   224   9756.6 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211   9756.6 MiB   9756.6 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218   9756.6 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219   9756.6 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220  11298.0 MiB   1541.4 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222  11298.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223  11298.0 MiB      0.0 MiB           1       if return_dataset:
   224  11298.0 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   211  11298.0 MiB  11298.0 MiB           1   @profile
   212                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   213                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   214                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   215                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   216                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, 
   217                                                                                add_fr_len=False, add_pep_len=False):
   218  11298.0 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   219  11298.0 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   220  11298.1 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   221                                             #TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   222  11298.1 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   223  11298.1 MiB      0.0 MiB           1       if return_dataset:
   224  11298.1 MiB      0.0 MiB           1           return dataloader, dataset
   225                                             else:
   226                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0368,	0.939
Epoch 50: valid loss, AUC:	0.0491,	0.911

Epoch 100: train loss, AUC:	0.0346,	0.942
Epoch 100: valid loss, AUC:	0.0489,	0.918

Epoch 150: train loss, AUC:	0.0336,	0.944
Epoch 150: valid loss, AUC:	0.0489,	0.915

Epoch 200: train loss, AUC:	0.0329,	0.945
Epoch 200: valid loss, AUC:	0.0491,	0.916

Epoch 250: train loss, AUC:	0.0324,	0.946
Epoch 250: valid loss, AUC:	0.0490,	0.915

Epoch 300: train loss, AUC:	0.0321,	0.946
Epoch 300: valid loss, AUC:	0.0499,	0.913

Epoch 350: train loss, AUC:	0.0317,	0.947
Epoch 350: valid loss, AUC:	0.0491,	0.917

Epoch 400: train loss, AUC:	0.0315,	0.947
Epoch 400: valid loss, AUC:	0.0489,	0.914

Epoch 450: train loss, AUC:	0.0314,	0.947
Epoch 450: valid loss, AUC:	0.0495,	0.912

Epoch 500: train loss, AUC:	0.0310,	0.947
Epoch 500: valid loss, AUC:	0.0504,	0.916
End of training cycles
Best train loss:	3.086e-02, best train AUC:	0.9478
Best valid epoch: 500
Best valid loss :	5.039e-02, best valid AUC:	0.9163
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None/checkpoint_best_kcv_MS_C00_data_reduced_0_f01_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   171  11298.1 MiB  11298.1 MiB           1   @profile
   172                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   173                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   174                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   175                                         
   176                                         
   177                                             Args:
   178                                                 n_epochs:
   179                                                 tolerance:
   180                                                 model:
   181                                                 criterion:
   182                                                 optimizer:
   183                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   184                                                 train_loader:
   185                                                 valid_loader:
   186                                                 checkpoint_filename:
   187                                                 outdir:
   188                                         
   189                                             Returns:
   190                                                 model
   191                                                 train_metrics
   192                                                 valid_metrics
   193                                                 train_losses
   194                                                 valid_losses
   195                                                 best_epoch
   196                                                 best_val_loss
   197                                                 best_val_auc
   198                                             """
   199  11298.1 MiB      0.0 MiB           1       if std == True:
   200                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   201                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   202                                                     xs = train_dataset[:][:-1]
   203                                                     print('Standardizing input data (including additional features)\n')
   204                                                     model.fit_standardizer(*xs)
   205                                             else:
   206  11298.1 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   207                                         
   208  11298.1 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   209  11298.1 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   210  11298.1 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   211  11307.3 MiB     -9.6 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   212  11307.3 MiB     -0.4 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   213  11307.0 MiB     -0.3 MiB           1           train_dataset.burn_in(False)
   214                                         
   215  11307.0 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   216                                             # Pre-saving the model at the very start because some bugged partitions
   217                                             # would have terrible performance and never save for very short debug runs.
   218  11307.1 MiB      0.0 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   219                                             # Actual runs
   220  11307.1 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   221  11307.1 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   222  12008.6 MiB -138417.4 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   223  12084.7 MiB -138435.7 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   224  12008.6 MiB -153826.8 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   225  12008.6 MiB -139119.8 MiB         500           train_metrics.append(train_metric)
   226  12008.6 MiB -139119.8 MiB         500           valid_metrics.append(valid_metric)
   227  12008.6 MiB -139119.8 MiB         500           train_losses.append(train_loss)
   228  12008.6 MiB -139119.8 MiB         500           valid_losses.append(valid_loss)
   229  12008.6 MiB -139119.8 MiB         500           if e % (n_epochs // 10) == 0:
   230                                                     # tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   231                                                     # tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   232  11309.8 MiB  -2831.1 MiB          10               print(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   233  11309.8 MiB    -31.7 MiB          10               print(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   234                                         
   235                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   236                                                 # performance for whatever reasons
   237                                         
   238                                                 # if e > 1 and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   239  12008.6 MiB -136320.3 MiB         500           if e == n_epochs:
   240  11306.4 MiB   -702.2 MiB           1               best_epoch = e
   241  11306.4 MiB      0.0 MiB           1               best_val_loss = valid_loss
   242  11306.4 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   243  11306.4 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   244                                         
   245  11306.4 MiB   -702.2 MiB           1       print(f'End of training cycles')
   246  11306.4 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   247  11306.4 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   248  11306.4 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   249  11306.4 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   250  11306.4 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   251  11306.4 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None/train_losses_kcv_MS_C00_data_reduced_0_f01_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None/valid_losses_kcv_MS_C00_data_reduced_0_f01_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None/train_metrics_kcv_MS_C00_data_reduced_0_f01_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None/valid_metrics_kcv_MS_C00_data_reduced_0_f01_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_1_231130_1045_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 468 minutes, 4 seconds.
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    350.5 MiB    350.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    350.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    350.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   9788.5 MiB   9438.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   9788.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   9788.5 MiB      0.0 MiB           1       if return_dataset:
   244   9788.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   9788.5 MiB   9788.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   9788.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   9788.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11312.2 MiB   1523.7 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11312.2 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11312.2 MiB      0.0 MiB           1       if return_dataset:
   244  11312.2 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231  11312.2 MiB  11312.2 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238  11312.2 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239  11312.2 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11312.2 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11312.2 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11312.2 MiB      0.0 MiB           1       if return_dataset:
   244  11312.2 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0366,	0.941
Epoch 50: valid loss, AUC:	0.0521,	0.916

Epoch 100: train loss, AUC:	0.0347,	0.944
Epoch 100: valid loss, AUC:	0.0496,	0.919

Epoch 150: train loss, AUC:	0.0334,	0.947
Epoch 150: valid loss, AUC:	0.0471,	0.919

Epoch 200: train loss, AUC:	0.0329,	0.947
Epoch 200: valid loss, AUC:	0.0494,	0.919

Epoch 250: train loss, AUC:	0.0322,	0.948
Epoch 250: valid loss, AUC:	0.0484,	0.920

Epoch 300: train loss, AUC:	0.0320,	0.948
Epoch 300: valid loss, AUC:	0.0475,	0.919

Epoch 350: train loss, AUC:	0.0315,	0.949
Epoch 350: valid loss, AUC:	0.0490,	0.917

Epoch 400: train loss, AUC:	0.0315,	0.949
Epoch 400: valid loss, AUC:	0.0478,	0.919

Epoch 450: train loss, AUC:	0.0315,	0.949
Epoch 450: valid loss, AUC:	0.0479,	0.918

Epoch 500: train loss, AUC:	0.0310,	0.949
Epoch 500: valid loss, AUC:	0.0493,	0.918
End of training cycles
Best train loss:	3.105e-02, best train AUC:	0.9495
Best valid epoch: 500
Best valid loss :	4.935e-02, best valid AUC:	0.9185
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None/checkpoint_best_kcv_MS_C00_data_reduced_0_f02_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170  11312.2 MiB  11312.2 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198  11312.2 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205  11312.2 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207  11312.2 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208  11312.2 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209  11312.2 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210  11334.4 MiB   -127.8 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211  11334.4 MiB   -105.7 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212  11321.8 MiB    -12.6 MiB           1           train_dataset.burn_in(False)
   213                                         
   214  11321.8 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217  11321.9 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219  11321.9 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220  11321.9 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221  11525.7 MiB -44833.9 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222  11562.9 MiB -44811.6 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223  11525.7 MiB -53200.1 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224  11525.7 MiB -45018.5 MiB         500           train_metrics.append(train_metric)
   225  11525.7 MiB -45018.5 MiB         500           valid_metrics.append(valid_metric)
   226  11525.7 MiB -45018.5 MiB         500           train_losses.append(train_loss)
   227  11525.7 MiB -45018.5 MiB         500           valid_losses.append(valid_loss)
   228                                         
   229  11525.7 MiB -45018.5 MiB         500           if (n_epochs)>10:
   230  11525.7 MiB -45018.5 MiB         500               if e % (n_epochs // 10) == 0:
   231  11341.1 MiB   -981.4 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232  11341.1 MiB    -12.5 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238  11525.7 MiB -44049.5 MiB         500           if e == n_epochs:
   239  11341.1 MiB   -184.5 MiB           1               best_epoch = e
   240  11341.1 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241  11341.1 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242  11341.1 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244  11341.1 MiB   -184.5 MiB           1       print(f'End of training cycles')
   245  11341.1 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246  11341.1 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247  11341.1 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248  11341.1 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249  11341.2 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250  11341.2 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None/train_losses_kcv_MS_C00_data_reduced_0_f02_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None/valid_losses_kcv_MS_C00_data_reduced_0_f02_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None/train_metrics_kcv_MS_C00_data_reduced_0_f02_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None/valid_metrics_kcv_MS_C00_data_reduced_0_f02_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_2_231130_1833_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 471 minutes, 47 seconds.
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    347.2 MiB    347.2 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    347.2 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    347.2 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   9772.9 MiB   9425.7 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   9772.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   9772.9 MiB      0.0 MiB           1       if return_dataset:
   244   9772.9 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   9772.9 MiB   9772.9 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   9772.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   9772.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11305.9 MiB   1533.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11305.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11305.9 MiB      0.0 MiB           1       if return_dataset:
   244  11305.9 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231  11305.9 MiB  11305.9 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238  11305.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239  11305.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11305.9 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11305.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11305.9 MiB      0.0 MiB           1       if return_dataset:
   244  11305.9 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0378,	0.935
Epoch 50: valid loss, AUC:	0.0451,	0.932

Epoch 100: train loss, AUC:	0.0357,	0.939
Epoch 100: valid loss, AUC:	0.0461,	0.930

Epoch 150: train loss, AUC:	0.0345,	0.941
Epoch 150: valid loss, AUC:	0.0466,	0.932

Epoch 200: train loss, AUC:	0.0337,	0.942
Epoch 200: valid loss, AUC:	0.0469,	0.933

Epoch 250: train loss, AUC:	0.0332,	0.943
Epoch 250: valid loss, AUC:	0.0447,	0.934

Epoch 300: train loss, AUC:	0.0325,	0.944
Epoch 300: valid loss, AUC:	0.0458,	0.933

Epoch 350: train loss, AUC:	0.0323,	0.945
Epoch 350: valid loss, AUC:	0.0449,	0.934

Epoch 400: train loss, AUC:	0.0321,	0.944
Epoch 400: valid loss, AUC:	0.0473,	0.933

Epoch 450: train loss, AUC:	0.0317,	0.945
Epoch 450: valid loss, AUC:	0.0447,	0.932

Epoch 500: train loss, AUC:	0.0318,	0.945
Epoch 500: valid loss, AUC:	0.0453,	0.931
End of training cycles
Best train loss:	3.159e-02, best train AUC:	0.9454
Best valid epoch: 500
Best valid loss :	4.529e-02, best valid AUC:	0.9313
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None/checkpoint_best_kcv_MS_C00_data_reduced_0_f03_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170  11305.9 MiB  11305.9 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198  11305.9 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205  11305.9 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207  11305.9 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208  11305.9 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209  11305.9 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210  11312.5 MiB     -8.2 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211  11312.5 MiB     -1.6 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212  11312.5 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214  11312.5 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217  11312.7 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219  11312.7 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220  11312.7 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221  11706.4 MiB -118617.9 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222  11705.6 MiB -119497.1 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223  11706.4 MiB -118489.0 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224  11706.4 MiB -119005.8 MiB         500           train_metrics.append(train_metric)
   225  11706.4 MiB -119005.8 MiB         500           valid_metrics.append(valid_metric)
   226  11706.4 MiB -119005.8 MiB         500           train_losses.append(train_loss)
   227  11706.4 MiB -119005.8 MiB         500           valid_losses.append(valid_loss)
   228                                         
   229  11706.4 MiB -119005.8 MiB         500           if (n_epochs)>10:
   230  11706.4 MiB -119005.8 MiB         500               if e % (n_epochs // 10) == 0:
   231  11333.9 MiB  -2546.2 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232  11333.9 MiB    -78.6 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238  11706.4 MiB -116538.3 MiB         500           if e == n_epochs:
   239  11318.4 MiB   -388.0 MiB           1               best_epoch = e
   240  11318.4 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241  11318.4 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242  11318.4 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244  11318.4 MiB   -388.0 MiB           1       print(f'End of training cycles')
   245  11318.4 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246  11318.4 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247  11318.4 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248  11318.4 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249  11318.5 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250  11318.5 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None/train_losses_kcv_MS_C00_data_reduced_0_f03_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None/valid_losses_kcv_MS_C00_data_reduced_0_f03_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None/train_metrics_kcv_MS_C00_data_reduced_0_f03_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None/valid_metrics_kcv_MS_C00_data_reduced_0_f03_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_3_231201_0225_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 476 minutes, 17 seconds.
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    347.3 MiB    347.3 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    347.3 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    347.3 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   9749.7 MiB   9402.4 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   9749.7 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   9749.7 MiB      0.0 MiB           1       if return_dataset:
   244   9749.7 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   9749.7 MiB   9749.7 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   9749.7 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   9749.7 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11296.5 MiB   1546.8 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11296.5 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11296.5 MiB      0.0 MiB           1       if return_dataset:
   244  11296.5 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231  11296.5 MiB  11296.5 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238  11296.5 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239  11296.5 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11296.6 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11296.6 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11296.6 MiB      0.0 MiB           1       if return_dataset:
   244  11296.6 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0366,	0.938
Epoch 50: valid loss, AUC:	0.0461,	0.926

Epoch 100: train loss, AUC:	0.0346,	0.941
Epoch 100: valid loss, AUC:	0.0467,	0.927

Epoch 150: train loss, AUC:	0.0334,	0.943
Epoch 150: valid loss, AUC:	0.0456,	0.928

Epoch 200: train loss, AUC:	0.0327,	0.943
Epoch 200: valid loss, AUC:	0.0476,	0.926

Epoch 250: train loss, AUC:	0.0318,	0.944
Epoch 250: valid loss, AUC:	0.0467,	0.926

Epoch 300: train loss, AUC:	0.0314,	0.945
Epoch 300: valid loss, AUC:	0.0466,	0.926

Epoch 350: train loss, AUC:	0.0310,	0.946
Epoch 350: valid loss, AUC:	0.0481,	0.926

Epoch 400: train loss, AUC:	0.0308,	0.946
Epoch 400: valid loss, AUC:	0.0470,	0.925

Epoch 450: train loss, AUC:	0.0307,	0.946
Epoch 450: valid loss, AUC:	0.0501,	0.926

Epoch 500: train loss, AUC:	0.0305,	0.947
Epoch 500: valid loss, AUC:	0.0467,	0.926
End of training cycles
Best train loss:	3.026e-02, best train AUC:	0.9473
Best valid epoch: 500
Best valid loss :	4.673e-02, best valid AUC:	0.9258
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None/checkpoint_best_kcv_MS_C00_data_reduced_0_f04_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170  11296.6 MiB  11296.6 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198  11296.6 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205  11296.6 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207  11296.6 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208  11296.6 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209  11296.6 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210  11308.4 MiB     -9.7 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211  11308.4 MiB      2.1 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212  11306.4 MiB     -2.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214  11306.4 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217  11306.6 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219  11306.6 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220  11306.6 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221  12148.0 MiB -244723.1 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222  12150.9 MiB -242976.7 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223  12148.0 MiB -252253.3 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224  12148.0 MiB -245565.7 MiB         500           train_metrics.append(train_metric)
   225  12148.0 MiB -245565.7 MiB         500           valid_metrics.append(valid_metric)
   226  12148.0 MiB -245565.7 MiB         500           train_losses.append(train_loss)
   227  12148.0 MiB -245565.7 MiB         500           valid_losses.append(valid_loss)
   228                                         
   229  12148.0 MiB -245565.7 MiB         500           if (n_epochs)>10:
   230  12148.0 MiB -245565.7 MiB         500               if e % (n_epochs // 10) == 0:
   231  11316.6 MiB  -5210.9 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232  11316.6 MiB    -94.5 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238  12148.0 MiB -240449.3 MiB         500           if e == n_epochs:
   239  11305.5 MiB   -842.5 MiB           1               best_epoch = e
   240  11305.5 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241  11305.5 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242  11305.5 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244  11305.5 MiB   -842.5 MiB           1       print(f'End of training cycles')
   245  11305.5 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246  11305.5 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247  11305.5 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248  11305.5 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249  11305.5 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250  11305.5 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None/train_losses_kcv_MS_C00_data_reduced_0_f04_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None/valid_losses_kcv_MS_C00_data_reduced_0_f04_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None/train_metrics_kcv_MS_C00_data_reduced_0_f04_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None/valid_metrics_kcv_MS_C00_data_reduced_0_f04_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_4_231201_1022_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 382 minutes, 16 seconds.
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231    350.1 MiB    350.1 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238    350.1 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239    350.1 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240   9750.9 MiB   9400.8 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242   9750.9 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243   9750.9 MiB      0.0 MiB           1       if return_dataset:
   244   9750.9 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231   9750.9 MiB   9750.9 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238   9750.9 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239   9750.9 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11299.0 MiB   1548.1 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11299.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11299.0 MiB      0.0 MiB           1       if return_dataset:
   244  11299.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/datasets.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   231  11299.0 MiB  11299.0 MiB           1   @profile
   232                                         def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',
   233                                                                                seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,
   234                                                                                indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,
   235                                                                                batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,
   236                                                                                add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,
   237                                                                                add_fr_len=False, add_pep_len=False):
   238  11299.0 MiB      0.0 MiB           1       dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,
   239  11299.0 MiB      0.0 MiB           1                                            burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,
   240  11299.0 MiB      0.0 MiB           1                                            add_fr_len, add_pep_len)
   241                                             # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC
   242  11299.0 MiB      0.0 MiB           1       dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))
   243  11299.0 MiB      0.0 MiB           1       if return_dataset:
   244  11299.0 MiB      0.0 MiB           1           return dataloader, dataset
   245                                             else:
   246                                                 return dataloader


No standardizing of the data

Doing burn-in period

Starting 500 training cycles

Epoch 50: train loss, AUC:	0.0365,	0.941
Epoch 50: valid loss, AUC:	0.0449,	0.922

Epoch 100: train loss, AUC:	0.0346,	0.943
Epoch 100: valid loss, AUC:	0.0444,	0.921

Epoch 150: train loss, AUC:	0.0338,	0.944
Epoch 150: valid loss, AUC:	0.0451,	0.921

Epoch 200: train loss, AUC:	0.0328,	0.946
Epoch 200: valid loss, AUC:	0.0455,	0.919

Epoch 250: train loss, AUC:	0.0322,	0.947
Epoch 250: valid loss, AUC:	0.0453,	0.919

Epoch 300: train loss, AUC:	0.0318,	0.948
Epoch 300: valid loss, AUC:	0.0466,	0.917

Epoch 350: train loss, AUC:	0.0318,	0.947
Epoch 350: valid loss, AUC:	0.0447,	0.920

Epoch 400: train loss, AUC:	0.0316,	0.948
Epoch 400: valid loss, AUC:	0.0474,	0.917

Epoch 450: train loss, AUC:	0.0311,	0.948
Epoch 450: valid loss, AUC:	0.0456,	0.920

Epoch 500: train loss, AUC:	0.0311,	0.949
Epoch 500: valid loss, AUC:	0.0451,	0.920
End of training cycles
Best train loss:	3.087e-02, best train AUC:	0.949
Best valid epoch: 500
Best valid loss :	4.506e-02, best valid AUC:	0.9203
Reloaded best model at /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None/checkpoint_best_kcv_MS_C00_data_reduced_0_f05_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None.pt
Filename: /home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/src/train_eval.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   170  11299.0 MiB  11299.0 MiB           1   @profile
   171                                         def train_eval_loops(n_epochs, tolerance, model, criterion, optimizer, train_dataset, train_loader, valid_loader,
   172                                                              checkpoint_filename, outdir, burn_in: int = None, std: bool = False):
   173                                             """ Trains and validates a model over n_epochs, then reloads the best checkpoint
   174                                         
   175                                         
   176                                             Args:
   177                                                 n_epochs:
   178                                                 tolerance:
   179                                                 model:
   180                                                 criterion:
   181                                                 optimizer:
   182                                                 train_dataset: Torch Dataset, is here so we can do standardize & burn-in periods
   183                                                 train_loader:
   184                                                 valid_loader:
   185                                                 checkpoint_filename:
   186                                                 outdir:
   187                                         
   188                                             Returns:
   189                                                 model
   190                                                 train_metrics
   191                                                 valid_metrics
   192                                                 train_losses
   193                                                 valid_losses
   194                                                 best_epoch
   195                                                 best_val_loss
   196                                                 best_val_auc
   197                                             """
   198  11299.0 MiB      0.0 MiB           1       if std == True:
   199                                                 if any([(hasattr(child, 'standardizer_sequence') or hasattr(child, 'ef_standardizer')) for child in [model.children()]+[model]]):
   200                                                     # TODO: Not sure about this workaround (works the same as in above with *data & pop(-1))
   201                                                     xs = train_dataset[:][:-1]
   202                                                     print('Standardizing input data (including additional features)\n')
   203                                                     model.fit_standardizer(*xs)
   204                                             else:
   205  11299.0 MiB      0.0 MiB           1           print('No standardizing of the data\n')
   206                                         
   207  11299.0 MiB      0.0 MiB           1       if burn_in is not None:  # doing burn-in if it's not None
   208  11299.0 MiB      0.0 MiB           1           print('Doing burn-in period\n')
   209  11299.0 MiB      0.0 MiB           1           train_dataset.burn_in(True)
   210  11305.8 MiB     -7.2 MiB          11           for e in tqdm(range(0, burn_in), desc='Burn-in period'):
   211  11305.8 MiB     -0.4 MiB          10               _, _ = train_model_step(model, criterion, optimizer, train_loader)
   212  11305.8 MiB      0.0 MiB           1           train_dataset.burn_in(False)
   213                                         
   214  11305.8 MiB      0.0 MiB           1       print(f'Starting {n_epochs} training cycles')
   215                                             # Pre-saving the model at the very start because some bugged partitions
   216                                             # would have terrible performance and never save for very short debug runs.
   217  11305.9 MiB      0.1 MiB           1       save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   218                                             # Actual runs
   219  11305.9 MiB      0.0 MiB           1       train_metrics, valid_metrics, train_losses, valid_losses = [], [], [], []
   220  11305.9 MiB      0.0 MiB           1       best_val_loss, best_val_auc, best_epoch = 1000, 0.5, 1
   221  11322.0 MiB   -615.5 MiB         501       for e in tqdm(range(1, n_epochs + 1), desc='epochs', leave=False):
   222  11322.0 MiB   -430.2 MiB         500           train_loss, train_metric = train_model_step(model, criterion, optimizer, train_loader)
   223  11322.0 MiB  -1048.0 MiB         500           valid_loss, valid_metric = eval_model_step(model, criterion, valid_loader)
   224  11322.0 MiB   -617.5 MiB         500           train_metrics.append(train_metric)
   225  11322.0 MiB   -617.5 MiB         500           valid_metrics.append(valid_metric)
   226  11322.0 MiB   -617.5 MiB         500           train_losses.append(train_loss)
   227  11322.0 MiB   -617.5 MiB         500           valid_losses.append(valid_loss)
   228                                         
   229  11322.0 MiB   -617.5 MiB         500           if (n_epochs)>10:
   230  11322.0 MiB   -617.5 MiB         500               if e % (n_epochs // 10) == 0:
   231  11320.1 MiB    -13.2 MiB          10                   tqdm.write(f'\nEpoch {e}: train loss, AUC:\t{train_loss:.4f},\t{train_metric["auc"]:.3f}')
   232  11320.1 MiB     -5.7 MiB          10                   tqdm.write(f'Epoch {e}: valid loss, AUC:\t{valid_loss:.4f},\t{valid_metric["auc"]:.3f}')
   233                                         
   234                                                 # Doesn't allow saving the very first model as sometimes it gets stuck in a random state that has good
   235                                                 # performance for whatever reasons
   236                                         
   237                                                 # and ((valid_loss <= best_val_loss + tolerance and valid_metric['auc'] > best_val_auc) or valid_metric['auc'] > best_val_auc):
   238  11322.0 MiB   -610.1 MiB         500           if e == n_epochs:
   239  11320.0 MiB     -2.0 MiB           1               best_epoch = e
   240  11320.0 MiB      0.0 MiB           1               best_val_loss = valid_loss
   241  11320.0 MiB      0.0 MiB           1               best_val_auc = valid_metric['auc']
   242  11320.0 MiB      0.0 MiB           1               save_checkpoint(model, filename=checkpoint_filename, dir_path=outdir)
   243                                         
   244  11320.0 MiB     -2.0 MiB           1       print(f'End of training cycles')
   245  11320.0 MiB      0.0 MiB         503       print(f'Best train loss:\t{min(train_losses):.3e}, best train AUC:\t{max([x["auc"] for x in train_metrics])}')
   246  11320.0 MiB      0.0 MiB           1       print(f'Best valid epoch: {best_epoch}')
   247  11320.0 MiB      0.0 MiB           1       print(f'Best valid loss :\t{best_val_loss:.3e}, best valid AUC:\t{best_val_auc}')
   248  11320.0 MiB      0.0 MiB           1       print(f'Reloaded best model at {os.path.abspath(os.path.join(outdir, checkpoint_filename))}')
   249  11320.1 MiB      0.0 MiB           1       model = load_checkpoint(model, checkpoint_filename, outdir)
   250  11320.1 MiB      0.0 MiB           1       return model, train_metrics, valid_metrics, train_losses, valid_losses, best_epoch, best_val_loss, best_val_auc


/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None/train_losses_kcv_MS_C00_data_reduced_0_f05_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None/valid_losses_kcv_MS_C00_data_reduced_0_f05_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None/train_metrics_kcv_MS_C00_data_reduced_0_f05_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None.pkl saved.
/home/projects/vaccine/people/cadsal/NNAlign_SpecialCourse/PyNNalign/output/CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None/valid_metrics_kcv_MS_C00_data_reduced_0_f05_CSL_hp_2hl_wd1e-4_bs256_MSdata0.15_KFold_5_231201_1644_None.pkl saved.
Reloading best model and returning validation and test predictions
Saving valid predictions from best model
Saving test predictions from best model
Program finished in 451 minutes, 23 seconds.
