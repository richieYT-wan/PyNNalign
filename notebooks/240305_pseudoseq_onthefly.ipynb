{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f01c9a0-99e9-476e-8e4c-835faf3cb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEsAAABWCAYAAACHBmuvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAABuvAAAbrwFeGpEcAAAEdElEQVR4nO2azStsfxzHX2PGnSLytPA8kzykRkixtFEof4GEIqUoGzsrayvGZjILG3lYiJKFIiULC5ntlIdhPOVhUGaI4bfQmXvdezHf2/ccnV/f12oWn+Z9evU9M99zvm/L6+vrK4q4SPjuCzATSpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYASpYAhsmanp6mrKyMubk5oyKlY9PjSyORCBMTEywtLREMBrHb7UQiET2iDEX6ygqHw3R0dOB2uwkGg+Tm5nJ/f080GgVgdXVVdqRhSJc1PDyMz+ejrKyM1tZWgsEgv56JrK6usrm5KTvWEKTKOjw8ZHFxEYvFwvX1NZOTkwAMDAyQl5cXmxsbG5MZaxhSZS0sLBCNRikuLubi4oKqqipmZ2fp7e19N7e9vc3JyYnMaEOQKmtnZweA6upqPB4PMzMzuFyudzNpaWkAbG1tyYw2BKmyAoEAABUVFdTX1/91RpN1cHAgM9oQpMq6uroCICMj48OZpKQkAEKhkMxoQ5Aq6+HhAYAfP358OGOz2d7NmgmpsqxWKwAWi+Xr4ATzPWlJvWLtFnt8fPxw5vn5GQC73S4z2hCkykpPTwfg5ubmw5lwOAxAZmamzGhDkCqrqKgIgGAw+OGM9sPudDplRhuCVFmVlZXAz/3W37i9vQXe9mJmQ6qspqYm4G3Dube39+FcbW0t+fn5MqMNQaosp9NJS0sL0WiU/v7+2CYVfv5WAX88/pgFi+yaZCgUor29Hb/fj9VqpbS0lLu7O46PjwFoaGhgfHxcZqRhSJcFb6vI6/WyvLzM0dERNpsNl8tFW1sbjY2NsuMMQxdZ/1fMt43+RpQsAZQsAZQsAZQsAZQsAZQsAZQsAUwpKxKJMDY2RlNTEy6Xi7q6Orq6ulhfX9e1U6FL10FPwuEwnZ2d+Hw+EhMTKSkp4ebmho2NDTY2NkhMTNQt23QrS6sHlJeXs7Kywvz8PGtra/T09ADw9PSkW7apVpZWD0hISGBkZIScnBweHx/xeDx4vV7d8021srR6QFVVFcXFxQQCARobG3G73QB0d3fHZj87B/hXTCVLe11dU1MDwNnZGaenp7FOxeDgYOw4bn9/X3q+qW5D7c1rYWEhANnZ2Xg8nndVAavVSjQa5fLyUnq+qWT9Xg9wOBw4HI53M9rh7a+vsWVhqtswnnqAdhqux7+iqWSJ1APimRHFVLLiqQdob8n12JyaSlY89YCXlxcAkpOTpeebSlY89QCtFZ2VlSU931SyvqoHnJ+fx2QVFBRIzzeVrK/qAVNTU7HPn7UP/xVTyfqsHrCwsMDExISu+abalAIMDQ3h9/vx+/00Nzf/UQ9ITU3l7u5Ol2xTrSx4+0ecmZmhr68Pp9PJ7u4uoVCI2tpaRkdHSUlJ0S1bHd8LYLqV9Z0oWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQIoWQL8B+8Ugojtu0b4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1.8x1.8 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "f,a = plt.subplots(1,1,figsize=(1e-2, 1e-2))\n",
    "mpl.rcParams['figure.dpi'] = 180\n",
    "sns.set_style('darkgrid')\n",
    "import os,sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "# Here you import other functions and classes\n",
    "from src.utils import mkdirs, convert_path, pkl_dump, pkl_load, display_side, add_median_labels, get_palette\n",
    "from src.data_processing import encode_batch, AA_KEYS, BL62_VALUES, BL62FREQ_VALUES, HLAS, BL50, BL50_VALUES\n",
    "from src.models import NNAlignEFSinglePass\n",
    "from src.datasets import NNAlignDatasetEFSinglePass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8a7d53-08b9-4d41-ad37-1d957172e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/mhc1_el_sub10k/sample_no_u.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f97b864-9bc7-4d21-9fa1-407b657e78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df200 = pd.read_csv('../data/mhc1_el_subsample/mhc1_el_200k_subsample.csv')\n",
    "df200['len'] = df200['sequence'].apply(len)\n",
    "df500 = pd.read_csv('../data/mhc1_el_subsample/mhc1_el_500k_subsample.csv')\n",
    "df500['len'] = df500['sequence'].apply(len)\n",
    "\n",
    "df200['flag'] = df200.apply(lambda x: any([z not in AA_KEYS for z in x['sequence']]), axis=1)\n",
    "df500['flag'] = df500.apply(lambda x: any([z not in AA_KEYS for z in x['sequence']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db317344-a630-4bd5-ad04-c0b7f0675095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>HLA</th>\n",
       "      <th>pseudoseq</th>\n",
       "      <th>fold</th>\n",
       "      <th>len</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>IVALILSTK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-A68:01</td>\n",
       "      <td>YGLIVASTKXXX</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>EENNSFQRL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-B44:03</td>\n",
       "      <td>PAEEENQRLSPX</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>KMKEALLSIGK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-A03:01</td>\n",
       "      <td>MTKKMKIGKXXX</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>VVNPKYEGK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-A03:01</td>\n",
       "      <td>VTTVVNEGKXXX</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>GANSKLTFGKG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HLA-B44:03</td>\n",
       "      <td>XYTGANGKGITL</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195692</th>\n",
       "      <td>GRLLIQPGPRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-B27:01</td>\n",
       "      <td>KPYGRLPRFHXX</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195894</th>\n",
       "      <td>YFDLWGRGTLVT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>XYWYFDLVTVSS</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196815</th>\n",
       "      <td>DGQKLLFARGTML</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HLA-B51:01</td>\n",
       "      <td>XFSDGQTMLKVD</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197104</th>\n",
       "      <td>YFDLWGRGTLVT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>XYWYFDLVTVSS</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197886</th>\n",
       "      <td>RLTDYVAFL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>QLARLTAFLENX</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sequence  target         HLA     pseudoseq  fold  len   flag\n",
       "635         IVALILSTK     1.0  HLA-A68:01  YGLIVASTKXXX     0    9  False\n",
       "975         EENNSFQRL     1.0  HLA-B44:03  PAEEENQRLSPX     0    9  False\n",
       "1057      KMKEALLSIGK     1.0  HLA-A03:01  MTKKMKIGKXXX     0   11  False\n",
       "2105        VVNPKYEGK     1.0  HLA-A03:01  VTTVVNEGKXXX     0    9  False\n",
       "2336      GANSKLTFGKG     0.0  HLA-B44:03  XYTGANGKGITL     0   11  False\n",
       "...               ...     ...         ...           ...   ...  ...    ...\n",
       "195692    GRLLIQPGPRF     1.0  HLA-B27:01  KPYGRLPRFHXX     4   11  False\n",
       "195894   YFDLWGRGTLVT     0.0  HLA-A24:02  XYWYFDLVTVSS     4   12  False\n",
       "196815  DGQKLLFARGTML     0.0  HLA-B51:01  XFSDGQTMLKVD     4   13  False\n",
       "197104   YFDLWGRGTLVT     0.0  HLA-A24:02  XYWYFDLVTVSS     4   12  False\n",
       "197886      RLTDYVAFL     1.0  HLA-A02:01  QLARLTAFLENX     4    9  False\n",
       "\n",
       "[277 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df200.query('pseudoseq.str.contains(\"X\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "02ced574-f6aa-4b5f-a1b7-5a60e135c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df200 = df200.query('not flag').rename(columns={'pseudoseq':'context'})\n",
    "df500 = df500.query('not flag').rename(columns={'pseudoseq':'context'})\n",
    "from src.data_processing import PSEUDOSEQDICT\n",
    "df200['pseudoseq'] = df200['HLA'].map(PSEUDOSEQDICT)\n",
    "df500['pseudoseq'] = df500['HLA'].map(PSEUDOSEQDICT)\n",
    "print(df500['pseudoseq'].isna().any())\n",
    "df200.to_csv('../data/mhc1_el_subsample/mhc1_el_200k_subsample.csv', index=False)\n",
    "df500.to_csv('../data/mhc1_el_subsample/mhc1_el_500k_subsample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae652a00-fc9a-43e4-bcef-0022d2c47c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.concat([df200.query('HLA==\"HLA-A02:01\"').sample(100, random_state=13), df200.query('HLA!=\"HLA-A02:01\"').sample(100, random_state=13)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224cff5-c4dd-4f06-a757-7ce634f45100",
   "metadata": {},
   "source": [
    "# testing on the fly batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17fc2000-e575-4980-981d-6742ef675a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "dataset = NNAlignDatasetEFSinglePass(sample, 13, 9, 'BL50LO', pad_scale=-20, add_pseudo_sequence=True, indel=True)\n",
    "loader = dataset.get_dataloader(50, SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "95cf6827-d3bc-4b12-ade4-12a33fdb2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffdccb-b87a-4b08-a076-70381333fae1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from src.data_processing import encode_batch, PFR_calculation, FR_lengths, pep_len_1hot, batch_insertion_deletion, batch_indel_mask\n",
    "# from memory_profiler import profile\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "class SuperDataset(Dataset):\n",
    "    def __init__(self, x=torch.empty([100, 1])):\n",
    "        super(SuperDataset, self).__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self\n",
    "\n",
    "    def get_dataloader(self, batch_size, sampler, **kwargs):\n",
    "        dataloader = DataLoader(self, batch_size=batch_size, sampler=sampler(self), **kwargs)\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "class TestDataset(SuperDataset):\n",
    "    \"\"\"\n",
    "    CLASS TO USE\n",
    "    \"\"\"\n",
    "\n",
    "    # @profile\n",
    "    def __init__(self, df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',\n",
    "                 seq_col: str = 'sequence', target_col: str = 'target', pad_scale: float = None, indel: bool = False,\n",
    "                 burnin_alphabet: str = 'ILVMFYW', feature_cols: list = ['placeholder'],\n",
    "                 add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False, add_fr_len=False,\n",
    "                 add_pep_len=False, add_z=True):\n",
    "        # start = dt.now()\n",
    "        super(NNAlignDatasetEFSinglePass, self).__init__()\n",
    "        # Encoding stuff\n",
    "        if feature_cols is None:\n",
    "            feature_cols = []\n",
    "        # Filter out sequences longer than max_len\n",
    "        df['len'] = df[seq_col].apply(len)\n",
    "        df = df.query('len<=@max_len')\n",
    "        # Then, if indel is False, filter out sequences shorter than windowsize (ex: 8mers for WS=9)\n",
    "        if not indel:\n",
    "            df = df.query('len>=@window_size')\n",
    "\n",
    "        matrix_dim = 20\n",
    "        # query_time = dt.now()\n",
    "        x = encode_batch(df[seq_col], max_len, encoding, pad_scale)\n",
    "        y = torch.from_numpy(df[target_col].values).float().view(-1, 1)\n",
    "        # encode_time = dt.now()\n",
    "        # Creating the mask to allow selection of kmers without padding\n",
    "        len_mask = torch.from_numpy(df['len'].values)\n",
    "        x_mask = len_mask - window_size\n",
    "        range_tensor = torch.arange(max_len - window_size + 1).unsqueeze(0).repeat(len(x), 1)\n",
    "        # Mask for Kmers + padding\n",
    "        x_mask = (range_tensor <= x_mask.unsqueeze(1)).float().unsqueeze(-1)\n",
    "        # Expand the kmers windows for base sequence without indels\n",
    "        x = x.unfold(1, window_size, 1).transpose(2, 3) \\\n",
    "             .reshape(len(x), max_len - window_size + 1, window_size, matrix_dim)\n",
    "        # Creating indels window and mask \n",
    "        if indel:\n",
    "            x_indel = batch_insertion_deletion(df[seq_col], max_len, encoding, pad_scale, window_size)\n",
    "            # remove padding from indel windows\n",
    "            x_indel = x_indel[:,:,:window_size, :]\n",
    "            indel_mask = batch_indel_mask(len_mask, window_size)\n",
    "            x = torch.cat([x, x_indel], dim=1)\n",
    "            x_mask = torch.cat([x_mask, indel_mask], dim=1)\n",
    "\n",
    "        # Creating another mask for the burn-in period+bool flag switch\n",
    "        self.burn_in_mask = _get_burnin_mask_batch(df[seq_col].values, max_len, window_size, burnin_alphabet).unsqueeze(\n",
    "            -1)\n",
    "        self.burn_in_flag = False\n",
    "\n",
    "        # Expand and unfold the sub kmers and the target to match the shape ; contiguous to allow for view operations\n",
    "        self.x_tensor = x.flatten(2, 3).contiguous()\n",
    "        self.x_mask = x_mask\n",
    "\n",
    "   \n",
    "        # kmer_time = dt.now()\n",
    "        self.y = y.contiguous()\n",
    "        self.x_features = torch.empty((len(x),))\n",
    "        # Add extra features\n",
    "        if len(feature_cols) > 0:\n",
    "            # TODO: When you add more features you need to concatenate to x_pseudosequence and save it to self.x_features\n",
    "            # these are NUMERICAL FEATURES like %Rank, expression, etc. of shape (N, len(feature_cols))\n",
    "            # x_features = torch.from_numpy(df[feature_cols].values).float()\n",
    "\n",
    "            self.extra_features_flag = True\n",
    "        else:\n",
    "            self.extra_features_flag = False\n",
    "\n",
    "        #  TODO dictmap for 9mer look-up and see if how many duplicated and can we save memory\n",
    "        #\n",
    "        if add_pseudo_sequence:\n",
    "            # TODO: Carlos, here you need to create the MHC feature vector and flatten it.\n",
    "            #       Basically, if you have the pseudo sequence in a column called 'pseudoseq' in your dataframe,\n",
    "            #       You can use my function encode_batch like\n",
    "            #       Do MHC pseudosequence as a dictionary and look-up on the fly in collate_fn to map back\n",
    "            #       only store dict and an ID for each datapoint\n",
    "            x_pseudoseq = encode_batch(df[pseudo_seq_col], 34, encoding, pad_scale)\n",
    "\n",
    "            # UNCOMMENT HERE WHEN YOU ARE DONE WITH THAT, check in a notebook that\n",
    "            # these dimension (N, 34*20) = (N, 680) are correct (you need to FLATTEN the vector using tensor.flatten(start_dim=1)\n",
    "            # then these should be working because my model forward() takes care of everything\n",
    "            x_pseudoseq = x_pseudoseq.flatten(start_dim=1)\n",
    "            self.x_features = x_pseudoseq\n",
    "            self.extra_features_flag = True\n",
    "            # ps_time = dt.now()\n",
    "        if add_pfr:\n",
    "            x_pfr = PFR_calculation(df[seq_col], self.x_mask, max_len, window_size)\n",
    "            self.x_tensor = torch.cat([self.x_tensor, x_pfr], dim=2)\n",
    "            # pfr_time = dt.now()\n",
    "        if add_fr_len:\n",
    "            x_fr_len = FR_lengths(self.x_mask, max_len, window_size)\n",
    "            self.x_tensor = torch.cat([self.x_tensor, x_fr_len], dim=2)\n",
    "            # pfr_len_time = dt.now()\n",
    "        if add_pep_len:\n",
    "            x_pep_len = pep_len_1hot(df[seq_col], max_len, window_size, min_length=13, max_length=21)\n",
    "            self.x_tensor = torch.cat([self.x_tensor, x_pep_len], dim=2)\n",
    "            # peplen_time = dt.now()\n",
    "\n",
    "        # Saving df in case it's needed\n",
    "        self.df = df\n",
    "        self.len = len(x)\n",
    "        self.max_len = max_len\n",
    "        self.seq_col = seq_col\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns the appropriate input tensors (X, ..., y) depending on the bool flags\n",
    "        A bit convoluted return, but basically 4 conditions:\n",
    "            1. No burn-in, no extra features --> returns the normal x_tensor, kmers mask, target\n",
    "            2. Burn-in, no extra features --> returns the normal x_tensor, burn-in mask, target\n",
    "            3. No Burn-in, + extra features --> returns the normal x_tensor, kmers mask, x_features, target\n",
    "            4. Burn-in, + extra features --> returns the normal x_tensor, burn-in mask, x_features, target\n",
    "        :param idx:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.burn_in_flag:\n",
    "            if self.extra_features_flag:\n",
    "                # 4\n",
    "                print(\n",
    "                    f'Tensor, Burn_in_mask, x_features, and y shapes: {self.x_tensor[idx].shape}, {self.burn_in_mask[idx].shape}, {self.x_features[idx].shape}, {self.y[idx].shape}')\n",
    "                return self.x_tensor[idx], self.burn_in_mask[idx], self.x_features[idx], self.y[idx]\n",
    "            else:\n",
    "                # 2\n",
    "                return self.x_tensor[idx], self.burn_in_mask[idx], self.y[idx]\n",
    "        else:\n",
    "            if self.extra_features_flag:\n",
    "                # 3\n",
    "                return self.x_tensor[idx], self.x_mask[idx], self.x_features[idx], self.y[idx]\n",
    "            else:\n",
    "                # 1\n",
    "                return self.x_tensor[idx], self.x_mask[idx], self.y[idx]\n",
    "\n",
    "    def burn_in(self, flag):\n",
    "        self.burn_in_flag = flag\n",
    "\n",
    "\n",
    "# @profile\n",
    "def get_NNAlign_dataloaderEFSinglePass(df: pd.DataFrame, max_len: int, window_size: int, encoding: str = 'onehot',\n",
    "                                       seq_col: str = 'Peptide', target_col: str = 'agg_label', pad_scale: float = None,\n",
    "                                       indel: bool = False, burnin_alphabet: str = 'ILVMFYW', feature_cols: list = None,\n",
    "                                       batch_size=64, sampler=torch.utils.data.RandomSampler, return_dataset=True,\n",
    "                                       add_pseudo_sequence=False, pseudo_seq_col: str = 'pseudoseq', add_pfr=False,\n",
    "                                       add_fr_len=False, add_pep_len=False):\n",
    "    dataset = NNAlignDatasetEFSinglePass(df, max_len, window_size, encoding, seq_col, target_col, pad_scale, indel,\n",
    "                                         burnin_alphabet, feature_cols, add_pseudo_sequence, pseudo_seq_col, add_pfr,\n",
    "                                         add_fr_len, add_pep_len)\n",
    "    # TODO NEW COLLATE FN ON THE FLY FOR KMERS AND MHC\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler(dataset))\n",
    "    if return_dataset:\n",
    "        return dataloader, dataset\n",
    "    else:\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "def _get_burnin_mask_batch(sequences, max_len, motif_len, alphabet='ILVMFYW'):\n",
    "    return torch.stack([_get_burnin_mask(x, max_len, motif_len, alphabet) for x in sequences])\n",
    "\n",
    "\n",
    "def _get_burnin_mask(seq, max_len, motif_len, alphabet='ILVMFYW'):\n",
    "    mask = torch.tensor([x in alphabet for i, x in enumerate(seq) if i < len(seq) - motif_len + 1]).float()\n",
    "    return F.pad(mask, (0, (max_len - motif_len + 1) - len(mask)), 'constant', 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pynn] *",
   "language": "python",
   "name": "conda-env-pynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
