{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0364700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "# from src.utils import pkl_load, pkl_dump\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f88d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>BA</th>\n",
       "      <th>MHC_name</th>\n",
       "      <th>pseudoseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DLDKKETVWHLEE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>HLA-DPA10103-DPB10201</td>\n",
       "      <td>YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HSLGKWLGHPDKF</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>HLA-DPA10103-DPB10201</td>\n",
       "      <td>YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VPDHVVWSLFNTL</td>\n",
       "      <td>0.871874</td>\n",
       "      <td>HLA-DPA10103-DPB10201</td>\n",
       "      <td>YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HTGREIVDLMCHAT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>HLA-DPA10103-DPB10201</td>\n",
       "      <td>YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAASVPAADKFKTFE</td>\n",
       "      <td>0.203668</td>\n",
       "      <td>HLA-DPA10103-DPB10201</td>\n",
       "      <td>YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106944</th>\n",
       "      <td>FEAQGAKANIAVD</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>H-2-IEk</td>\n",
       "      <td>QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106945</th>\n",
       "      <td>EEDIEIIPIQEEEY</td>\n",
       "      <td>0.072792</td>\n",
       "      <td>H-2-IEk</td>\n",
       "      <td>QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106946</th>\n",
       "      <td>AAHSAAFEDLRVSSY</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>H-2-IEk</td>\n",
       "      <td>QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106947</th>\n",
       "      <td>YAGIRRDGLLLRLVD</td>\n",
       "      <td>0.441317</td>\n",
       "      <td>H-2-IEk</td>\n",
       "      <td>QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106948</th>\n",
       "      <td>QVPLVQQQQYLGQQQP</td>\n",
       "      <td>0.450627</td>\n",
       "      <td>H-2-IEk</td>\n",
       "      <td>QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106949 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Sequence        BA               MHC_name  \\\n",
       "0          DLDKKETVWHLEE  0.000000  HLA-DPA10103-DPB10201   \n",
       "1          HSLGKWLGHPDKF  0.000000  HLA-DPA10103-DPB10201   \n",
       "2          VPDHVVWSLFNTL  0.871874  HLA-DPA10103-DPB10201   \n",
       "3         HTGREIVDLMCHAT  0.000000  HLA-DPA10103-DPB10201   \n",
       "4        AAASVPAADKFKTFE  0.203668  HLA-DPA10103-DPB10201   \n",
       "...                  ...       ...                    ...   \n",
       "106944     FEAQGAKANIAVD  0.000000                H-2-IEk   \n",
       "106945    EEDIEIIPIQEEEY  0.072792                H-2-IEk   \n",
       "106946   AAHSAAFEDLRVSSY  0.052982                H-2-IEk   \n",
       "106947   YAGIRRDGLLLRLVD  0.441317                H-2-IEk   \n",
       "106948  QVPLVQQQQYLGQQQP  0.450627                H-2-IEk   \n",
       "\n",
       "                                 pseudoseq  \n",
       "0       YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT  \n",
       "1       YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT  \n",
       "2       YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT  \n",
       "3       YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT  \n",
       "4       YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT  \n",
       "...                                    ...  \n",
       "106944  QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL  \n",
       "106945  QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL  \n",
       "106946  QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL  \n",
       "106947  QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL  \n",
       "106948  QEFFIASGAAVDAVMECSLVYFDFQKETVHIFFL  \n",
       "\n",
       "[106949 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Train_1_MHCps.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8ffc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sequence, max_len=None, encoding='onehot', pad_scale=None):\n",
    "    \"\"\"\n",
    "    encodes a single peptide into a matrix, using 'onehot' or 'blosum'\n",
    "    if 'blosum', then need to provide the blosum dictionary as argument\n",
    "    \"\"\"\n",
    "    assert encoding in encoding_matrix_dict.keys(), f'Wrong encoding key {encoding} passed!'\\\n",
    "                                                    f'Should be any of {encoding_matrix_dict.keys()}'\n",
    "    # One hot encode by setting 1 to positions where amino acid is present, 0 elsewhere\n",
    "    size = len(sequence)\n",
    "    blosum_matrix = encoding_matrix_dict[encoding]\n",
    "    if encoding == 'onehot':\n",
    "        int_encoded = [CHAR_TO_INT[char] for char in sequence]\n",
    "        onehot_encoded = list()\n",
    "        for value in int_encoded:\n",
    "            letter = [0 for _ in range(len(AA_KEYS))]\n",
    "            letter[value] = 1 if value != -1 else 0\n",
    "            onehot_encoded.append(letter)\n",
    "        tmp = np.array(onehot_encoded)\n",
    "\n",
    "    # BLOSUM encode\n",
    "    else:\n",
    "        if blosum_matrix is None or not isinstance(blosum_matrix, dict):\n",
    "            raise Exception('No BLOSUM matrix provided!')\n",
    "\n",
    "        tmp = np.zeros([size, len(AA_KEYS)], dtype=np.float32)\n",
    "        for idx in range(size):\n",
    "            # Here, the way Morten takes cares of Xs is to leave it blank, i.e. as zeros\n",
    "            # So only use blosum matrix to encode if sequence[idx] != 'X'\n",
    "            if sequence[idx]!='X':\n",
    "                tmp[idx, :] = blosum_matrix[sequence[idx]]\n",
    "\n",
    "\n",
    "    # Padding if max_len is provided\n",
    "    if max_len is not None and max_len > size:\n",
    "        diff = int(max_len) - int(size)\n",
    "        try:\n",
    "            if pad_scale is None:\n",
    "                pad_scale = 0 if encoding=='onehot' else -12\n",
    "            tmp = np.concatenate([tmp, pad_scale*np.ones([diff, len(AA_KEYS)], dtype=np.float32)],\n",
    "                                 axis=0)\n",
    "        except:\n",
    "            print('Here in encode', type(tmp), tmp.shape, len(AA_KEYS), type(diff), type(max_len), type(size), sequence)\n",
    "            #     return tmp, diff, len(AA_KEYS)\n",
    "            raise Exception\n",
    "    return torch.from_numpy(tmp).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eaaefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(sequences, max_len=None, encoding='onehot', pad_scale=None):\n",
    "    \"\"\"\n",
    "    Encode multiple sequences at once.\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max([len(x) for x in sequences])\n",
    "    # Contiguous to allow for .view operation\n",
    "    return torch.stack([encode(seq, max_len, encoding, pad_scale) for seq in sequences]).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91284c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/Users/minij/OneDrive/Documentos/GitHub/PyNNalign/data/'\n",
    "OUTDIR = '/Users/minij/OneDrive/Documentos/Try_output/'\n",
    "# Stupid hardcoded variable\n",
    "CNN_FEATS = ['EL_ratio', 'anchor_mutation', 'delta_VHSE1', 'delta_VHSE3', 'delta_VHSE7', 'delta_VHSE8',\n",
    "             'delta_aliphatic_index',\n",
    "             'delta_boman', 'delta_hydrophobicity', 'delta_isoelectric_point', 'delta_rank']\n",
    "\n",
    "\n",
    "def _init(DATADIR):\n",
    "    #### ==== CONST (blosum, multiprocessing, keys, etc) ==== ####\n",
    "    VAL = math.floor(4 + (multiprocessing.cpu_count() / 1.5))\n",
    "    N_CORES = VAL if VAL <= multiprocessing.cpu_count() else int(multiprocessing.cpu_count() - 2)\n",
    "\n",
    "    MATRIXDIR = f'{DATADIR}Matrices/'\n",
    "    ICSDIR = f'{DATADIR}ic_dicts/'\n",
    "    AA_KEYS = [x for x in 'ARNDCQEGHILKMFPSTWYV']\n",
    "\n",
    "    CHAR_TO_INT = dict((c, i) for i, c in enumerate(AA_KEYS))\n",
    "    INT_TO_CHAR = dict((i, c) for i, c in enumerate(AA_KEYS))\n",
    "    CHAR_TO_INT['X'] = -1\n",
    "    CHAR_TO_INT['-'] = -1\n",
    "    INT_TO_CHAR[-1] = '-'\n",
    "\n",
    "    BG = np.loadtxt(f'{MATRIXDIR}bg.freq.fmt', dtype=float)\n",
    "    BG = dict((k, v) for k, v in zip(AA_KEYS, BG))\n",
    "\n",
    "    # BLOSUMS 50\n",
    "    BL50 = {}\n",
    "    _blosum50 = np.loadtxt(f'{MATRIXDIR}BLOSUM50', dtype=float).T\n",
    "    for i, letter_1 in enumerate(AA_KEYS):\n",
    "        BL50[letter_1] = {}\n",
    "        for j, letter_2 in enumerate(AA_KEYS):\n",
    "            BL50[letter_1][letter_2] = _blosum50[i, j]\n",
    "    BL50_VALUES = {k: np.array([v for v in BL50[k].values()]) for k in BL50}\n",
    "    # BLOSUMS 62\n",
    "    BL62_DF = pd.read_csv(f'{MATRIXDIR}BLOSUM62', sep='\\s+', comment='#', index_col=0)\n",
    "    BL62 = BL62_DF.to_dict()\n",
    "    BL62_VALUES = BL62_DF.drop(columns=['B', 'Z', 'X', '*'], index=['B', 'Z', 'X', '*'])\n",
    "    BL62_VALUES = dict((x, BL62_VALUES.loc[x].values) for x in BL62_VALUES.index)\n",
    "\n",
    "    # BLOSUMS 62 FREQS\n",
    "    _blosum62 = np.loadtxt(f'{MATRIXDIR}BLOSUM62.freq_rownorm', dtype=float).T\n",
    "    BL62FREQ = {}\n",
    "    BL62FREQ_VALUES = {}\n",
    "    for i, letter_1 in enumerate(AA_KEYS):\n",
    "        BL62FREQ[letter_1] = {}\n",
    "        BL62FREQ_VALUES[letter_1] = _blosum62[i]\n",
    "        for j, letter_2 in enumerate(AA_KEYS):\n",
    "            BL62FREQ[letter_1][letter_2] = _blosum62[i, j]\n",
    "    #ICS_KL = pkl_load(ICSDIR + 'ics_kl_new.pkl')\n",
    "    #ICS_SHANNON = pkl_load(ICSDIR + 'ics_shannon.pkl')\n",
    "    #HLAS = ICS_SHANNON[9].keys()\n",
    "\n",
    "    #return VAL, N_CORES, DATADIR, AA_KEYS, CHAR_TO_INT, INT_TO_CHAR, BG, BL62FREQ, BL62FREQ_VALUES, BL50, BL50_VALUES, BL62, BL62_VALUES, HLAS, ICS_KL, ICS_SHANNON\n",
    "    return VAL, N_CORES, DATADIR, AA_KEYS, CHAR_TO_INT, INT_TO_CHAR, BG, BL62FREQ, BL62FREQ_VALUES, BL50, BL50_VALUES, BL62, BL62_VALUES\n",
    "\n",
    "\n",
    "#VAL, N_CORES, DATADIR, AA_KEYS, CHAR_TO_INT, INT_TO_CHAR, BG, BL62FREQ, BL62FREQ_VALUES, BL50, BL50_VALUES, BL62, BL62_VALUES, HLAS, ICS_KL, ICS_SHANNON = _init(DATADIR)\n",
    "VAL, N_CORES, DATADIR, AA_KEYS, CHAR_TO_INT, INT_TO_CHAR, BG, BL62FREQ, BL62FREQ_VALUES, BL50, BL50_VALUES, BL62, BL62_VALUES = _init(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdb6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_matrix_dict = {'onehot': None,\n",
    "                        'BL62LO': BL62_VALUES,\n",
    "                        'BL62FREQ': BL62FREQ_VALUES,\n",
    "                        'BL50LO': BL50_VALUES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a08e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = 'BL50LO'\n",
    "pad_scale = None\n",
    "df['len'] = df['Sequence'].apply(len)\n",
    "window_size = 9\n",
    "burnin_alphabet = 'ILVMFYW'\n",
    "matrix_dim = 20\n",
    "feature_cols = ['pseudoseq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6adb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "max_len = df['len'].max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae038ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_burnin_mask(seq, max_len, motif_len, alphabet='ILVMFYW'):\n",
    "    mask = torch.tensor([x in alphabet for i, x in enumerate(seq) if i < len(seq) - motif_len + 1]).float()\n",
    "    return F.pad(mask, (0, (max_len - motif_len + 1) - len(mask)), 'constant', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47c5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_burnin_mask_batch(sequences, max_len, motif_len, alphabet='ILVMFYW'):\n",
    "    return torch.stack([_get_burnin_mask(x, max_len, motif_len, alphabet) for x in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e580a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X DIMENSIONS:\n",
    "#     - Total number of sequences of the dataset (106949 sequences)\n",
    "#     - Max-length padding: all of the sequences have been truncated to have the max-length (35 positions)\n",
    "#     - One-hot encoding of the aa: 20 possible values of the BLOSUM encoding for each position in the sequence (20 aa)\n",
    "\n",
    "\n",
    "x = encode_batch(df['Sequence'], max_len, encoding, pad_scale)\n",
    "y = torch.from_numpy(df['BA'].values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b97ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106949, 35, 20])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e5ee9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dara original dimensions:\n",
      "torch.Size([106949, 35, 20]) \n",
      "\n",
      "X_tensor dimensions:\n",
      "torch.Size([106949, 27, 180]) \n",
      "\n",
      "X_pseudoseq dimensions after the first encoding:\n",
      "torch.Size([106949, 34, 20]) \n",
      "\n",
      "X_pseudoseq dimensions after FLATTEN the vector:\n",
      "torch.Size([106949, 680]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = encode_batch(df['Sequence'], max_len, encoding, pad_scale)\n",
    "y = torch.from_numpy(df['BA'].values).float().view(-1, 1)\n",
    "\n",
    "print('X_dara original dimensions:')\n",
    "print(x.shape, '\\n')\n",
    "\n",
    "# Creating the mask to allow selection of kmers without padding\n",
    "x_mask = torch.from_numpy(df['len'].values) - window_size\n",
    "range_tensor = torch.arange(max_len - window_size + 1).unsqueeze(0).repeat(len(x), 1)\n",
    "# Mask for Kmers + padding\n",
    "x_mask = (range_tensor <= x_mask.unsqueeze(1)).float().unsqueeze(-1)\n",
    "# Creating another mask for the burn-in period+bool flag switch\n",
    "burn_in_mask = _get_burnin_mask_batch(df['Sequence'].values, max_len, window_size, burnin_alphabet).unsqueeze(-1)\n",
    "burn_in_flag = False\n",
    "# Expand and unfold the sub kmers and the target to match the shape ; contiguous to allow for view operations\n",
    "x_tensor = x.unfold(1, window_size, 1).transpose(2, 3) \\\n",
    "    .reshape(len(x), max_len - window_size + 1, window_size, matrix_dim).flatten(2, 3).contiguous()\n",
    "y = y.contiguous()\n",
    "\n",
    "print('X_tensor dimensions:')\n",
    "print(x_tensor.shape, '\\n')\n",
    "\n",
    "# Add extra features\n",
    "if len(feature_cols) > 0:\n",
    "    # TODO: When you add more features you need to concatenate to x_pseudosequence and save it to self.x_features\n",
    "    # these are NUMERICAL FEATURES like %Rank, expression, etc. of shape (N, len(feature_cols))\n",
    "    # x_features = torch.from_numpy(df[feature_cols].values).float()\n",
    "\n",
    "    extra_features_flag = True\n",
    "else:\n",
    "    x_features = torch.empty((len(x),))\n",
    "    extra_features_flag = False\n",
    "\n",
    "# TODO: Carlos, here you need to create the MHC feature vector and flatten it.\n",
    "#       Basically, if you have the pseudo sequence in a column called 'pseudoseq' in your dataframe,\n",
    "#       You can use my function encode_batch like\n",
    "x_pseudoseq = encode_batch(df['pseudoseq'], 34, encoding, pad_scale)\n",
    "print('X_pseudoseq dimensions after the first encoding:')\n",
    "print(x_pseudoseq.shape, '\\n')\n",
    "    \n",
    "\n",
    "# UNCOMMENT HERE WHEN YOU ARE DONE WITH THAT, check in a notebook that\n",
    "# these dimension (N, 34*20) = (N, 680) are correct (you need to FLATTEN the vector using tensor.flatten(start_dim=1)\n",
    "# then these should be working because my model forward() takes care of everything\n",
    "    \n",
    "x_pseudoseq = x_pseudoseq.flatten(start_dim=1)\n",
    "print('X_pseudoseq dimensions after FLATTEN the vector:')\n",
    "print(x_pseudoseq.shape, '\\n')\n",
    "x_features = x_pseudoseq\n",
    "extra_features_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5316ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the first sequence in X_tensor:\n",
      "     - 27 as the number of possible sequnces due to the sliding window (35-9+1)\n",
      "     - 180 as the number of 20 encoded aminoacids multiplied by the 9 length of the binding motif\n",
      "\n",
      "torch.Size([27, 180])\n",
      "tensor([[ -2.,  -2.,   2.,  ...,  15.,   2.,  -3.],\n",
      "        [ -2.,  -3.,  -4.,  ...,  -3.,   2.,  -4.],\n",
      "        [ -2.,  -2.,   2.,  ...,  -2.,  -1.,   1.],\n",
      "        ...,\n",
      "        [-12., -12., -12.,  ..., -12., -12., -12.],\n",
      "        [-12., -12., -12.,  ..., -12., -12., -12.],\n",
      "        [-12., -12., -12.,  ..., -12., -12., -12.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of the first sequence in X_tensor:\")\n",
    "print(\"     - 27 as the number of possible sequnces due to the sliding window (35-9+1)\")\n",
    "print(\"     - 180 as the number of 20 encoded aminoacids multiplied by the 9 length of the binding motif\\n\")\n",
    "print(x_tensor[0].shape)\n",
    "print(x_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9dfd53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 9mer sequences in the training set:\n",
      "torch.Size([2887623, 180])\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of 9mer sequences in the training set:\")\n",
    "print(x_tensor.flatten(start_dim=0, end_dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b4e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the x_mask tensor (repeated 1 for each sequence 27 times according to the sliding window):\n",
      "torch.Size([106949, 27, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the x_mask tensor (repeated 1 for each sequence 27 times according to the sliding window):\")\n",
    "print(x_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438d30f",
   "metadata": {},
   "source": [
    "## Individual PFR with masks (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "880cd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the mean position values of the flanking regions\n",
    "def calculate_mean_flanking_positions(seq, mask, window_size=9):\n",
    "    \n",
    "    # Define output\n",
    "    all_prev_pfr = []\n",
    "    all_after_pfr = []\n",
    "    \n",
    "    # Previous PFR mask definition\n",
    "    PFR_mask_before = 3 * torch.ones((27, 1))\n",
    "    PFR_mask_before[:3,0] = torch.tensor([0, 1, 2], dtype=torch.float32)   # First three elements to 1 and 2\n",
    "    \n",
    "    # After PFR mask definition (according to their x_mask)\n",
    "    PFR_mask_after = torch.clone(mask)*3\n",
    "\n",
    "    # Modification of the previous values before 0\n",
    "    PFR_mask_after[torch.where(PFR_mask_after == 0)[0][0]-3] = 2\n",
    "    PFR_mask_after[torch.where(PFR_mask_after == 0)[0][0]-2] = 1\n",
    "    PFR_mask_after[torch.where(PFR_mask_after == 0)[0][0]-1] = 0\n",
    "    \n",
    "    for i in range(0, seq.shape[0] - window_size + 1, 1):\n",
    "        \n",
    "        # Define the WS-mer\n",
    "        peptide = seq[i:i + window_size]\n",
    "        \n",
    "        # Store the resulting PFR tensors\n",
    "        all_prev_pfr.append(torch.sum(seq[i-int(PFR_mask_before[i]):i], dim=0, keepdim=True)/3)\n",
    "        all_after_pfr.append(torch.sum(seq[i+window_size:i+window_size+int(PFR_mask_after[i])], dim=0, keepdim=True)/3)\n",
    "    \n",
    "    print(f\"PFR mask for the 'previous' amino acids:\\n {PFR_mask_before}\")\n",
    "    print(f\"PFR mask for the 'after' amino acids:\\n {PFR_mask_after}\")\n",
    "    \n",
    "    return all_prev_pfr, all_after_pfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c43a5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFR mask for the 'previous' amino acids:\n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.]])\n",
      "PFR mask for the 'after' amino acids:\n",
      " tensor([[3.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "seq1 = x[0,:]\n",
    "x_mask1 = x_mask[0,:]\n",
    "prev, after = calculate_mean_flanking_positions(seq1, x_mask1, window_size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ea8fbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous PFR for the first sequence:\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[-0.6667, -0.6667,  0.6667,  2.6667, -1.3333,  0.0000,  0.6667, -0.3333,\n",
      "         -0.3333, -1.3333, -1.3333, -0.3333, -1.3333, -1.6667, -0.3333,  0.0000,\n",
      "         -0.3333, -1.6667, -1.0000, -1.3333]])\n",
      "\n",
      "tensor([[-1.3333, -1.6667, -0.6667,  1.3333, -2.0000, -0.6667, -0.3333, -1.6667,\n",
      "         -1.3333, -0.6667,  0.3333, -1.3333, -0.3333, -1.3333, -1.6667, -1.0000,\n",
      "         -0.6667, -2.3333, -1.3333, -1.0000]])\n",
      "\n",
      "tensor([[-2.0000, -2.3333,  0.0000,  4.0000, -3.3333, -0.6667,  0.3333, -2.0000,\n",
      "         -1.6667, -2.0000, -1.0000, -1.6667, -1.6667, -3.0000, -2.0000, -1.0000,\n",
      "         -1.0000, -4.0000, -2.3333, -2.3333]])\n",
      "\n",
      "tensor([[-1.6667, -0.6667, -0.6667,  1.0000, -3.0000,  0.0000,  0.0000, -2.3333,\n",
      "         -1.3333, -1.6667, -0.6667,  0.6667, -1.0000, -2.6667, -2.0000, -1.0000,\n",
      "         -1.0000, -3.3333, -2.0000, -2.0000]])\n",
      "\n",
      "tensor([[-1.3333,  1.3333,  0.6667,  2.0000, -3.3333,  1.3333,  1.3333, -1.6667,\n",
      "         -0.3333, -3.3333, -3.3333,  3.6667, -2.6667, -4.3333, -1.0000,  0.0000,\n",
      "         -1.0000, -3.6667, -2.3333, -3.3333]])\n",
      "\n",
      "tensor([[-1.0000,  2.0000,  0.0000,  0.0000, -3.0000,  2.0000,  2.6667, -2.3333,\n",
      "          0.0000, -3.3333, -3.0000,  4.3333, -2.0000, -3.6667, -1.0000, -0.3333,\n",
      "         -1.0000, -3.0000, -2.0000, -3.0000]])\n",
      "\n",
      "tensor([[-0.6667,  0.6667,  0.0000,  0.0000, -2.3333,  1.0000,  2.0000, -2.3333,\n",
      "         -0.6667, -2.6667, -2.3333,  2.0000, -1.6667, -3.0000, -1.0000,  0.3333,\n",
      "          1.0000, -3.0000, -2.0000, -2.0000]])\n",
      "\n",
      "tensor([[-0.3333, -1.3333, -1.0000, -1.0000, -1.6667, -0.6667,  0.6667, -3.0000,\n",
      "         -2.0000, -0.3333, -1.0000, -1.0000, -0.6667, -2.0000, -1.6667, -0.3333,\n",
      "          1.3333, -3.0000, -1.6667,  0.6667]])\n",
      "\n",
      "tensor([[-1.0000, -2.3333, -2.3333, -3.3333, -2.3333, -1.6667, -2.3333, -3.0000,\n",
      "         -3.0000,  0.0000, -0.6667, -2.3333, -0.3333, -0.6667, -2.6667, -1.3333,\n",
      "          0.6667,  3.0000, -0.3333,  0.6667]])\n",
      "\n",
      "tensor([[-1.6667, -2.0000, -2.0000, -3.3333, -3.0000, -1.0000, -2.0000, -3.0000,\n",
      "          1.0000, -1.0000, -1.3333, -2.0000, -0.3333, -0.3333, -3.0000, -2.3333,\n",
      "         -1.6667,  3.0000,  1.0000, -0.6667]])\n",
      "\n",
      "tensor([[-2.3333, -2.0000, -2.3333, -3.3333, -3.3333, -0.6667, -2.0000, -3.0000,\n",
      "          1.3333, -1.6667,  0.0000, -2.0000,  0.3333,  0.3333, -3.3333, -2.6667,\n",
      "         -2.0000,  3.3333,  1.0000, -2.0000]])\n",
      "\n",
      "tensor([[-1.6667, -1.0000, -1.0000, -1.0000, -2.6667,  0.3333,  1.0000, -3.0000,\n",
      "          2.3333, -2.0000, -0.3333, -0.6667,  0.0000, -1.0000, -2.3333, -1.6667,\n",
      "         -1.3333, -2.6667, -0.3333, -2.0000]])\n",
      "\n",
      "tensor([[-1.3333, -1.0000, -1.3333,  0.0000, -2.6667,  0.6667,  3.0000, -3.3333,\n",
      "         -1.0000, -2.0000, -0.3333, -0.3333, -0.3333, -1.6667, -2.0000, -1.6667,\n",
      "         -1.0000, -2.6667, -1.6667, -1.6667]])\n",
      "\n",
      "tensor([[-4.6667, -4.0000, -4.0000, -2.6667, -6.0000, -2.6667,  0.0000, -6.0000,\n",
      "         -4.0000, -6.6667, -6.0000, -3.3333, -5.3333, -6.0000, -4.6667, -4.6667,\n",
      "         -4.6667, -6.0000, -5.3333, -6.0000]])\n",
      "\n",
      "tensor([[-8.3333, -8.0000, -8.0000, -7.3333, -9.0000, -7.3333, -6.0000, -9.0000,\n",
      "         -8.0000, -9.3333, -9.0000, -7.6667, -8.6667, -9.0000, -8.3333, -8.3333,\n",
      "         -8.3333, -9.0000, -8.6667, -9.0000]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n",
      "tensor([[-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.,\n",
      "         -12., -12., -12., -12., -12., -12., -12., -12.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Previous PFR for the first sequence:\\n')\n",
    "\n",
    "for element in prev:\n",
    "    print(element)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b5c3050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PFR for the first sequence:\n",
      "\n",
      "tensor([[-1.6667, -1.0000, -1.0000, -1.0000, -2.6667,  0.3333,  1.0000, -3.0000,\n",
      "          2.3333, -2.0000, -0.3333, -0.6667,  0.0000, -1.0000, -2.3333, -1.6667,\n",
      "         -1.3333, -2.6667, -0.3333, -2.0000]])\n",
      "\n",
      "tensor([[-1.3333, -1.0000, -1.3333,  0.0000, -2.6667,  0.6667,  3.0000, -3.3333,\n",
      "         -1.0000, -2.0000, -0.3333, -0.3333, -0.3333, -1.6667, -2.0000, -1.6667,\n",
      "         -1.0000, -2.6667, -1.6667, -1.6667]])\n",
      "\n",
      "tensor([[-0.6667,  0.0000,  0.0000,  1.3333, -2.0000,  1.3333,  4.0000, -2.0000,\n",
      "          0.0000, -2.6667, -2.0000,  0.6667, -1.3333, -2.0000, -0.6667, -0.6667,\n",
      "         -0.6667, -2.0000, -1.3333, -2.0000]])\n",
      "\n",
      "tensor([[-0.3333,  0.0000,  0.0000,  0.6667, -1.0000,  0.6667,  2.0000, -1.0000,\n",
      "          0.0000, -1.3333, -1.0000,  0.3333, -0.6667, -1.0000, -0.3333, -0.3333,\n",
      "         -0.3333, -1.0000, -0.6667, -1.0000]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('After PFR for the first sequence:\\n')\n",
    "\n",
    "for element in after:\n",
    "    print(element)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9afbad2",
   "metadata": {},
   "source": [
    "## Individual PFR with masks (tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d09c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the mean position values of the flanking regions\n",
    "def calculate_mean_flanking_positions_tensor(seq, mask, window_size=9):\n",
    "    \n",
    "    # Define output\n",
    "    all_pfr = torch.empty((seq.shape[0] - window_size + 1, 2, seq.shape[1]))  # Initialize the output tensor\n",
    "    \n",
    "    # Previous PFR mask definition\n",
    "    PFR_mask_before = 3 * torch.ones((27, 1))\n",
    "    PFR_mask_before[:3, 0] = torch.tensor([0, 1, 2], dtype=torch.float32)   # First three elements to 1 and 2\n",
    "    \n",
    "    # After PFR mask definition (according to their x_mask)\n",
    "    PFR_mask_after = torch.clone(mask) * 3\n",
    "\n",
    "    # Modification of the previous values before 0 (only if zero exists)\n",
    "    zero_indices = torch.where(PFR_mask_after == 0)[0]\n",
    "    if zero_indices.numel() > 0:  # Check if there are any zero indices\n",
    "        zero_index = zero_indices[0]  # Get the first zero index\n",
    "        PFR_mask_after[zero_index - 3] = 2 if zero_index >= 3 else PFR_mask_after[zero_index - 3]\n",
    "        PFR_mask_after[zero_index - 2] = 1 if zero_index >= 2 else PFR_mask_after[zero_index - 2]\n",
    "        PFR_mask_after[zero_index - 1] = 0 if zero_index >= 1 else PFR_mask_after[zero_index - 1]\n",
    "\n",
    "    \n",
    "    for i in range(0, seq.shape[0] - window_size + 1, 1):\n",
    "        \n",
    "        # Define the WS-mer\n",
    "        peptide = seq[i:i + window_size]\n",
    "        \n",
    "        # Store the resulting PFR tensors\n",
    "        prev_pfr = torch.sum(seq[i - int(PFR_mask_before[i]):i], dim=0, keepdim=True) / 3\n",
    "        after_pfr = torch.sum(seq[i + window_size:i + window_size + int(PFR_mask_after[i])], dim=0, keepdim=True) / 3\n",
    "\n",
    "        # Store the PFR tensors in the output tensor\n",
    "        all_pfr[i, 0] = prev_pfr\n",
    "        all_pfr[i, 1] = after_pfr\n",
    "    \n",
    "    return all_pfr.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466f3447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 40])\n",
      "tensor([[  0.0000,   0.0000,   0.0000,  ...,  -2.6667,  -0.3333,  -2.0000],\n",
      "        [ -0.6667,  -0.6667,   0.6667,  ...,  -2.6667,  -1.6667,  -1.6667],\n",
      "        [ -1.3333,  -1.6667,  -0.6667,  ...,  -2.0000,  -1.3333,  -2.0000],\n",
      "        ...,\n",
      "        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "seq1 = x[0,:]\n",
    "x_mask1 = x_mask[0,:]\n",
    "result = calculate_mean_flanking_positions_tensor(seq1, x_mask1, window_size=9)\n",
    "\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6259a5b",
   "metadata": {},
   "source": [
    "## All data PFR with masks (tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6a20b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the mean position values of the flanking regions\n",
    "def PFR_calculation(data, all_xmask, max_len, window_size=9):\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define output\n",
    "    all_pfr = torch.empty((data.shape[0], max_len - window_size + 1, 2, 20))  # Initialize the output tensor\n",
    "    \n",
    "    for j in range(0, data.shape[0], 1):\n",
    "        \n",
    "        seq = data[j,:]\n",
    "    \n",
    "        # Previous PFR mask definition\n",
    "        PFR_mask_before = 3 * torch.ones((max_len - window_size + 1, 1))\n",
    "        PFR_mask_before[:3,0] = torch.tensor([0, 1, 2], dtype=torch.float32)   # First three elements to 1 and 2\n",
    "\n",
    "        # After PFR mask definition (according to their x_mask)\n",
    "        PFR_mask_after = torch.clone(all_xmask[j])*3\n",
    "\n",
    "        # Modification of the previous values before 0 (only if zero exists)\n",
    "        zero_indices = torch.where(PFR_mask_after == 0)[0]\n",
    "        if zero_indices.numel() > 0:  # Check if there are any zero indices\n",
    "            zero_index = zero_indices[0]  # Get the first zero index\n",
    "            PFR_mask_after[zero_index - 3] = 2 if zero_index >= 3 else PFR_mask_after[zero_index - 3]\n",
    "            PFR_mask_after[zero_index - 2] = 1 if zero_index >= 2 else PFR_mask_after[zero_index - 2]\n",
    "            PFR_mask_after[zero_index - 1] = 0 if zero_index >= 1 else PFR_mask_after[zero_index - 1]\n",
    "    \n",
    "        for i in range(0, seq.shape[0] - window_size + 1, 1):\n",
    "\n",
    "            # Define the WS-mer\n",
    "            peptide = seq[i:i + window_size]\n",
    "\n",
    "            # Store the resulting PFR tensors\n",
    "            prev_pfr = torch.sum(seq[i - int(PFR_mask_before[i]):i], dim=0, keepdim=True) / 3\n",
    "            after_pfr = torch.sum(seq[i + window_size:i + window_size + int(PFR_mask_after[i])], dim=0, keepdim=True) / 3\n",
    "\n",
    "            # Store the PFR tensors in the output tensor\n",
    "            all_pfr[j, i, 0] = prev_pfr\n",
    "            all_pfr[j, i, 1] = after_pfr\n",
    "            \n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function took {elapsed_time:.4f} seconds to run.\")\n",
    "    \n",
    "    return all_pfr.flatten(start_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c7f7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function took 290.0358 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "result = PFR_calculation(x, x_mask, max_len, window_size=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787ffcc",
   "metadata": {},
   "source": [
    "Around 4,83 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aedccbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([106949, 27, 40]),\n",
       " tensor([[  0.0000,   0.0000,   0.0000,  ...,  -2.6667,  -0.3333,  -2.0000],\n",
       "         [ -0.6667,  -0.6667,   0.6667,  ...,  -2.6667,  -1.6667,  -1.6667],\n",
       "         [ -1.3333,  -1.6667,  -0.6667,  ...,  -2.0000,  -1.3333,  -2.0000],\n",
       "         ...,\n",
       "         [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [-12.0000, -12.0000, -12.0000,  ...,   0.0000,   0.0000,   0.0000]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape, result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa96746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
